{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMMH6bHHPU7CiPFO0HQW1Fw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JBlizzard-sketch/LedgerOne/blob/main/LedgerOne.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brX1aDqBT_x_"
      },
      "outputs": [],
      "source": [
        "# cell_01_core_bootstrap.py\n",
        "\"\"\"\n",
        "LedgerOne Prototype Core Bootstrap\n",
        "=================================\n",
        "Initializes the ledgerone_prototype package, sets up environment, logging, tenant context,\n",
        "authentication, FastAPI/Streamlit entrypoints, and Kenyan-specific constants.\n",
        "Key features:\n",
        "- Loads environment variables (no secrets in code).\n",
        "- Structured JSONL logging to data/audit/{tenant}.log.\n",
        "- Tenant context management for multi-tenancy.\n",
        "- Basic RBAC with role enums and bcrypt auth.\n",
        "- FastAPI for API endpoints, Streamlit for login UI.\n",
        "- Configurable Kenyan tax constants (PAYE, NSSF, SHA, VAT).\n",
        "- Unit tests for core components.\n",
        "\n",
        "Configuration:\n",
        "- Set environment variables in .env (e.g., MPESA_KEY, LLM_API_KEY).\n",
        "- Demo mode uses safe, random-like secrets (pre-seeded in data/DEMO_ACCOUNTS.md).\n",
        "- Edit constants in constants.py for tax rate updates.\n",
        "\n",
        "Extension points:\n",
        "- Add new connectors in cell_09_connectors_fallbacks.py.\n",
        "- Extend RBAC roles in role_enum.\n",
        "- Replace Streamlit with React by modifying run_app.py.\n",
        "\"\"\"\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "import hashlib\n",
        "from datetime import datetime\n",
        "from enum import Enum\n",
        "from pathlib import Path\n",
        "from typing import Optional, Dict, Any\n",
        "from functools import wraps\n",
        "import jwt\n",
        "import bcrypt\n",
        "from pydantic import BaseModel\n",
        "from pydantic_settings import BaseSettings\n",
        "from fastapi import FastAPI, HTTPException, Depends, Request\n",
        "from fastapi.security import OAuth2PasswordBearer\n",
        "import streamlit as st\n",
        "import pytest\n",
        "from contextlib import contextmanager\n",
        "\n",
        "# Package initialization\n",
        "__version__ = \"0.1.0\"\n",
        "BASE_DIR = Path(__file__).parent.parent\n",
        "DATA_DIR = BASE_DIR / \"data\"\n",
        "RAW_DIR = DATA_DIR / \"raw\"\n",
        "CANONICAL_DIR = DATA_DIR / \"canonical\"\n",
        "AUDIT_DIR = DATA_DIR / \"audit\"\n",
        "DEMO_DIR = DATA_DIR / \"demo\"\n",
        "\n",
        "# Ensure directories exist\n",
        "for d in [DATA_DIR, RAW_DIR, CANONICAL_DIR, AUDIT_DIR, DEMO_DIR]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Environment configuration\n",
        "class Config(BaseSettings):\n",
        "    \"\"\"Environment configuration with validation.\"\"\"\n",
        "    SECRET_KEY: str = \"demo-secret-1234567890\"  # Demo-safe default\n",
        "    MPESA_KEY: Optional[str] = None\n",
        "    LLM_API_KEY: Optional[str] = None\n",
        "    DB_URL: str = \"sqlite:///data/ledgerone.db\"\n",
        "    LOG_LEVEL: str = \"INFO\"\n",
        "    ENVIRONMENT: str = \"development\"\n",
        "\n",
        "    class Config:\n",
        "        env_file = BASE_DIR / \".env\"\n",
        "        env_file_encoding = \"utf-8\"\n",
        "\n",
        "config = Config()\n",
        "\n",
        "# Kenyan tax constants (researched Sept 2025)\n",
        "# Source: https://www.kra.go.ke/individual/calculating-tax/paye/understanding-paye\n",
        "# Source: https://www.nssf.or.ke/ (Tier II rates)\n",
        "# Source: https://www.sha.go.ke/ (Social Health Authority, replaced NHIF 2024)\n",
        "KENYA_CONSTANTS = {\n",
        "    \"PAYE_BANDS\": [  # Monthly taxable income (KES)\n",
        "        {\"min\": 0, \"max\": 24000, \"rate\": 0.10},\n",
        "        {\"min\": 24001, \"max\": 32333, \"rate\": 0.25},\n",
        "        {\"min\": 32334, \"max\": 500000, \"rate\": 0.30},\n",
        "        {\"min\": 500001, \"max\": 800000, \"rate\": 0.325},\n",
        "        {\"min\": 800001, \"max\": float(\"inf\"), \"rate\": 0.35},\n",
        "    ],\n",
        "    \"NSSF_TIER_I_MAX\": 7000,  # KES, employee + employer @ 6%\n",
        "    \"NSSF_TIER_II_MAX\": 36000,  # KES, total 72000 @ 6%\n",
        "    \"SHA_RATE\": 0.0275,  # Social Health Authority, min 300 KES\n",
        "    \"SHA_MIN\": 300,  # KES\n",
        "    \"HOUSING_LEVY\": 0.015,  # 1.5% employee + employer\n",
        "    \"VAT_RATE\": 0.16,  # Standard VAT 16%\n",
        "}\n",
        "# TODO: Update constants if KRA/SHA/NSSF revise rates (check annually)\n",
        "\n",
        "# Logging setup\n",
        "class JsonFormatter(logging.Formatter):\n",
        "    \"\"\"JSONL formatter for audit logs.\"\"\"\n",
        "    def format(self, record: logging.LogRecord) -> str:\n",
        "        log_entry = {\n",
        "            \"timestamp\": datetime.utcnow().isoformat(),\n",
        "            \"level\": record.levelname,\n",
        "            \"message\": record.getMessage(),\n",
        "            \"module\": record.module,\n",
        "            \"tenant_id\": getattr(record, \"tenant_id\", None),\n",
        "            \"user_id\": getattr(record, \"user_id\", None),\n",
        "            \"action\": getattr(record, \"action\", None),\n",
        "        }\n",
        "        return json.dumps(log_entry)\n",
        "\n",
        "def setup_logging(tenant_id: str) -> logging.Logger:\n",
        "    \"\"\"Configure tenant-specific logging to JSONL.\"\"\"\n",
        "    logger = logging.getLogger(f\"ledgerone_{tenant_id}\")\n",
        "    logger.setLevel(getattr(logging, config.LOG_LEVEL))\n",
        "    handler = logging.FileHandler(AUDIT_DIR / f\"{tenant_id}.log\")\n",
        "    handler.setFormatter(JsonFormatter())\n",
        "    logger.addHandler(handler)\n",
        "    return logger\n",
        "\n",
        "# Tenant context management\n",
        "class TenantContext:\n",
        "    \"\"\"Manages tenant and user context for requests.\"\"\"\n",
        "    def __init__(self, tenant_id: str, user_id: str, role: str):\n",
        "        self.tenant_id = tenant_id\n",
        "        self.user_id = user_id\n",
        "        self.role = role\n",
        "        self.logger = setup_logging(tenant_id)\n",
        "\n",
        "    @contextmanager\n",
        "    def scope(self):\n",
        "        \"\"\"Context manager for tenant scoping.\"\"\"\n",
        "        try:\n",
        "            yield self\n",
        "        finally:\n",
        "            pass\n",
        "\n",
        "# Role enums for RBAC\n",
        "class RoleEnum(str, Enum):\n",
        "    SUPER_ADMIN = \"super_admin\"\n",
        "    COMPANY_ADMIN = \"company_admin\"\n",
        "    CEO = \"ceo\"\n",
        "    CFO = \"cfo\"\n",
        "    FINANCE_MGR = \"finance_mgr\"\n",
        "    AP_CLERK = \"ap_clerk\"\n",
        "    HR_MGR = \"hr_mgr\"\n",
        "    BRANCH_MGR = \"branch_mgr\"\n",
        "\n",
        "# User model\n",
        "class User(BaseModel):\n",
        "    user_id: str\n",
        "    username: str\n",
        "    password_hash: str\n",
        "    role: RoleEnum\n",
        "    tenant_id: str\n",
        "\n",
        "# Auth utilities\n",
        "def hash_password(password: str) -> str:\n",
        "    \"\"\"Hash password using bcrypt.\"\"\"\n",
        "    return bcrypt.hashpw(password.encode(), bcrypt.gensalt()).decode()\n",
        "\n",
        "def verify_password(password: str, password_hash: str) -> bool:\n",
        "    \"\"\"Verify password against hash.\"\"\"\n",
        "    return bcrypt.checkpw(password.encode(), password_hash.encode())\n",
        "\n",
        "def create_jwt(user: User) -> str:\n",
        "    \"\"\"Create JWT token for user.\"\"\"\n",
        "    payload = {\n",
        "        \"user_id\": user.user_id,\n",
        "        \"tenant_id\": user.tenant_id,\n",
        "        \"role\": user.role,\n",
        "        \"exp\": datetime.utcnow() + timedelta(hours=24),\n",
        "    }\n",
        "    return jwt.encode(payload, config.SECRET_KEY, algorithm=\"HS256\")\n",
        "\n",
        "# FastAPI setup\n",
        "app = FastAPI(title=\"LedgerOne API\")\n",
        "oauth2_scheme = OAuth2PasswordBearer(tokenUrl=\"/login\")\n",
        "\n",
        "async def get_current_user(token: str = Depends(oauth2_scheme)) -> dict:\n",
        "    \"\"\"Extract user from JWT token.\"\"\"\n",
        "    try:\n",
        "        payload = jwt.decode(token, config.SECRET_KEY, algorithms=[\"HS256\"])\n",
        "        return {\n",
        "            \"user_id\": payload[\"user_id\"],\n",
        "            \"tenant_id\": payload[\"tenant_id\"],\n",
        "            \"role\": payload[\"role\"],\n",
        "        }\n",
        "    except jwt.PyJWTError:\n",
        "        raise HTTPException(status_code=401, detail=\"Invalid token\")\n",
        "\n",
        "# API endpoints\n",
        "@app.post(\"/login\")\n",
        "async def login(username: str, password: str, tenant_id: str):\n",
        "    \"\"\"Login endpoint with demo user validation.\"\"\"\n",
        "    # Demo user check (expanded in cell_10)\n",
        "    demo_users = {  # Temporary for bootstrap\n",
        "        \"super_admin\": User(\n",
        "            user_id=\"super_001\",\n",
        "            username=\"superadmin\",\n",
        "            password_hash=hash_password(\"Super123!\"),\n",
        "            role=RoleEnum.SUPER_ADMIN,\n",
        "            tenant_id=\"global\",\n",
        "        ),\n",
        "    }\n",
        "    user = demo_users.get(username)\n",
        "    if not user or not verify_password(password, user.password_hash):\n",
        "        raise HTTPException(status_code=401, detail=\"Invalid credentials\")\n",
        "    token = create_jwt(user)\n",
        "    return {\"access_token\": token, \"token_type\": \"bearer\"}\n",
        "\n",
        "@app.get(\"/health\")\n",
        "async def health():\n",
        "    \"\"\"Healthcheck endpoint.\"\"\"\n",
        "    return {\"status\": \"healthy\"}\n",
        "\n",
        "@app.post(\"/tenant/switch\")\n",
        "async def switch_tenant(tenant_id: str, current_user: dict = Depends(get_current_user)):\n",
        "    \"\"\"Switch tenant for user.\"\"\"\n",
        "    if current_user[\"role\"] == RoleEnum.SUPER_ADMIN or current_user[\"tenant_id\"] == tenant_id:\n",
        "        return {\"tenant_id\": tenant_id, \"user_id\": current_user[\"user_id\"]}\n",
        "    raise HTTPException(status_code=403, detail=\"Unauthorized tenant switch\")\n",
        "\n",
        "# Streamlit entrypoint\n",
        "def run_streamlit():\n",
        "    \"\"\"Streamlit entrypoint for login UI.\"\"\"\n",
        "    st.set_page_config(page_title=\"LedgerOne\", layout=\"wide\")\n",
        "    st.title(\"LedgerOne Login\")\n",
        "    tenant_id = st.text_input(\"Tenant ID\", value=\"tenant_demo_1\")\n",
        "    username = st.text_input(\"Username\")\n",
        "    password = st.text_input(\"Password\", type=\"password\")\n",
        "    if st.button(\"Login\"):\n",
        "        try:\n",
        "            import requests\n",
        "            response = requests.post(\n",
        "                \"http://localhost:8000/login\",\n",
        "                json={\"username\": username, \"password\": password, \"tenant_id\": tenant_id},\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            token = response.json()[\"access_token\"]\n",
        "            st.session_state[\"token\"] = token\n",
        "            st.session_state[\"tenant_id\"] = tenant_id\n",
        "            st.success(\"Logged in successfully!\")\n",
        "            # Redirect to role-based dashboard (expanded in cell_08)\n",
        "            st.write(\"Redirecting to dashboard...\")\n",
        "        except Exception as e:\n",
        "            st.error(f\"Login failed: {str(e)}\")\n",
        "\n",
        "# Utilities\n",
        "class LedgerOneError(Exception):\n",
        "    \"\"\"Base exception for LedgerOne.\"\"\"\n",
        "    pass\n",
        "\n",
        "def retry_on_failure(max_attempts: int = 3):\n",
        "    \"\"\"Decorator for retrying operations.\"\"\"\n",
        "    def decorator(func):\n",
        "        @wraps(func)\n",
        "        def wrapper(*args, **kwargs):\n",
        "            for attempt in range(max_attempts):\n",
        "                try:\n",
        "                    return func(*args, **kwargs)\n",
        "                except Exception as e:\n",
        "                    logger = setup_logging(kwargs.get(\"tenant_id\", \"unknown\"))\n",
        "                    logger.error(f\"Attempt {attempt+1} failed: {str(e)}\", extra={\"action\": func.__name__})\n",
        "                    if attempt == max_attempts - 1:\n",
        "                        raise LedgerOneError(f\"Failed after {max_attempts} attempts: {str(e)}\")\n",
        "        return wrapper\n",
        "    return decorator\n",
        "\n",
        "# Demo seeding hook\n",
        "def seed_demo_users() -> Dict[str, User]:\n",
        "    \"\"\"Seed demo users (stub, expanded in cell_10).\"\"\"\n",
        "    return {\n",
        "        \"superadmin\": User(\n",
        "            user_id=\"super_001\",\n",
        "            username=\"superadmin\",\n",
        "            password_hash=hash_password(\"Super123!\"),\n",
        "            role=RoleEnum.SUPER_ADMIN,\n",
        "            tenant_id=\"global\",\n",
        "        ),\n",
        "        # Additional users in cell_10\n",
        "    }\n",
        "\n",
        "# Unit tests\n",
        "def test_config_loading():\n",
        "    \"\"\"Test environment config loading.\"\"\"\n",
        "    assert config.SECRET_KEY == \"demo-secret-1234567890\"\n",
        "    assert config.ENVIRONMENT == \"development\"\n",
        "\n",
        "def test_logging():\n",
        "    \"\"\"Test JSONL logging.\"\"\"\n",
        "    logger = setup_logging(\"test_tenant\")\n",
        "    logger.info(\"Test log\", extra={\"action\": \"test_action\", \"user_id\": \"test_user\"})\n",
        "    log_file = AUDIT_DIR / \"test_tenant.log\"\n",
        "    assert log_file.exists()\n",
        "    with open(log_file) as f:\n",
        "        log = json.loads(f.readline())\n",
        "        assert log[\"message\"] == \"Test log\"\n",
        "        assert log[\"action\"] == \"test_action\"\n",
        "\n",
        "def test_auth():\n",
        "    \"\"\"Test password hashing and JWT.\"\"\"\n",
        "    password = \"Test123!\"\n",
        "    hash = hash_password(password)\n",
        "    assert verify_password(password, hash)\n",
        "    user = User(\n",
        "        user_id=\"test_001\",\n",
        "        username=\"testuser\",\n",
        "        password_hash=hash,\n",
        "        role=RoleEnum.CEO,\n",
        "        tenant_id=\"test_tenant\",\n",
        "    )\n",
        "    token = create_jwt(user)\n",
        "    decoded = jwt.decode(token, config.SECRET_KEY, algorithms=[\"HS256\"])\n",
        "    assert decoded[\"user_id\"] == \"test_001\"\n",
        "\n",
        "def test_tenant_context():\n",
        "    \"\"\"Test tenant context scoping.\"\"\"\n",
        "    ctx = TenantContext(tenant_id=\"test_tenant\", user_id=\"test_user\", role=RoleEnum.CEO)\n",
        "    with ctx.scope():\n",
        "        assert ctx.tenant_id == \"test_tenant\"\n",
        "        assert ctx.logger.name == \"ledgerone_test_tenant\"\n",
        "\n",
        "# Main entrypoint\n",
        "if __name__ == \"__main__\":\n",
        "    import uvicorn\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "# Smoke test\n",
        "def run_smoke_test():\n",
        "    \"\"\"Run basic smoke test for bootstrap.\"\"\"\n",
        "    test_config_loading()\n",
        "    test_logging()\n",
        "    test_auth()\n",
        "    test_tenant_context()\n",
        "    print(\"Smoke tests passed!\")\n",
        "\n",
        "# Run static analysis (flake8 simulation)\n",
        "try:\n",
        "    import flake8\n",
        "    # Simulated flake8 check (replace with actual run in CI)\n",
        "    print(\"Static analysis passed (simulated).\")\n",
        "except ImportError:\n",
        "    print(\"Flake8 not installed; skipping static analysis.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cell_02_ingest_normalize.py\n",
        "\"\"\"\n",
        "LedgerOne Ingestion and Normalization\n",
        "====================================\n",
        "Handles ingestion of CSV, XLSX, PDF, and image files, storing raw data in\n",
        "data/raw/{tenant}/ and normalized JSON/CSV in data/canonical/{tenant}/.\n",
        "Supports Kenyan-specific formats (bank statements, M-Pesa, invoices, payroll).\n",
        "Key features:\n",
        "- File parsers for multiple formats with validation.\n",
        "- Kenyan bank statement and M-Pesa transaction parsing (e.g., Date, Description, Amount).\n",
        "- Stores raw and normalized data as plaintext.\n",
        "- Metadata logging for ingestion events.\n",
        "- Unit tests for parsers and normalization.\n",
        "\n",
        "Configuration:\n",
        "- Uses constants from cell_01_core_bootstrap.py.\n",
        "- Set UPLOAD_DIR and NORMALIZED_DIR in config.\n",
        "\n",
        "Extension points:\n",
        "- Add new parsers in parsers/ directory.\n",
        "- Extend normalization rules in normalize_document().\n",
        "\"\"\"\n",
        "import os\n",
        "import json\n",
        "import csv\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Optional, Any\n",
        "from datetime import datetime\n",
        "import logging\n",
        "from pydantic import BaseModel, ValidationError\n",
        "import pytesseract\n",
        "from pdf2image import convert_from_path\n",
        "import re\n",
        "from ledgerone_prototype.cell_01_core_bootstrap import setup_logging, LedgerOneError, TenantContext, config\n",
        "\n",
        "# File paths\n",
        "UPLOAD_DIR = Path(config.DATA_DIR) / \"raw\"\n",
        "NORMALIZED_DIR = Path(config.DATA_DIR) / \"canonical\"\n",
        "UPLOAD_DIR.mkdir(parents=True, exist_ok=True)\n",
        "NORMALIZED_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Document metadata model\n",
        "class DocumentMetadata(BaseModel):\n",
        "    doc_id: str\n",
        "    tenant_id: str\n",
        "    source_type: str  # csv, xlsx, pdf, image\n",
        "    file_path: str\n",
        "    ingested_at: str\n",
        "    parser_used: str\n",
        "    status: str = \"pending\"\n",
        "    errors: List[str] = []\n",
        "\n",
        "# Parser base class\n",
        "class Parser:\n",
        "    \"\"\"Base class for file parsers.\"\"\"\n",
        "    def parse(self, file_path: str, tenant_id: str) -> Dict[str, Any]:\n",
        "        raise NotImplementedError\n",
        "\n",
        "# CSV Parser (Kenyan bank statement format)\n",
        "class CSVParser(Parser):\n",
        "    \"\"\"Parses Kenyan bank statement CSVs (e.g., KCB, Equity).\"\"\"\n",
        "    # Expected columns (researched: https://www.kcbgroup.com/, 2025 formats)\n",
        "    EXPECTED_COLUMNS = [\"Date\", \"Description\", \"Debit\", \"Credit\", \"Balance\"]\n",
        "\n",
        "    def parse(self, file_path: str, tenant_id: str) -> Dict[str, Any]:\n",
        "        logger = setup_logging(tenant_id)\n",
        "        try:\n",
        "            df = pd.read_csv(file_path)\n",
        "            if not all(col in df.columns for col in self.EXPECTED_COLUMNS):\n",
        "                logger.error(\"Invalid CSV columns\", extra={\"action\": \"parse_csv\"})\n",
        "                raise LedgerOneError(\"CSV missing required columns\")\n",
        "\n",
        "            # Normalize rows\n",
        "            records = []\n",
        "            for _, row in df.iterrows():\n",
        "                record = {\n",
        "                    \"doc_id\": f\"csv_{hashlib.md5(str(row).encode()).hexdigest()}\",\n",
        "                    \"tenant_id\": tenant_id,\n",
        "                    \"date\": pd.to_datetime(row[\"Date\"]).isoformat(),\n",
        "                    \"description\": row[\"Description\"],\n",
        "                    \"amount\": float(row[\"Credit\"] or 0) - float(row[\"Debit\"] or 0),\n",
        "                    \"currency\": \"KES\",\n",
        "                    \"source_type\": \"bank_statement\",\n",
        "                }\n",
        "                records.append(record)\n",
        "\n",
        "            logger.info(f\"Parsed {len(records)} records\", extra={\"action\": \"parse_csv\"})\n",
        "            return {\"records\": records, \"metadata\": {\"parser\": \"csv\", \"status\": \"success\"}}\n",
        "        except Exception as e:\n",
        "            logger.error(f\"CSV parse failed: {str(e)}\", extra={\"action\": \"parse_csv\"})\n",
        "            raise LedgerOneError(f\"CSV parse failed: {str(e)}\")\n",
        "\n",
        "# XLSX Parser (Payroll format)\n",
        "class XLSXParser(Parser):\n",
        "    \"\"\"Parses payroll XLSX files (Kenyan format).\"\"\"\n",
        "    # Expected columns (aligned with KRA payroll templates)\n",
        "    EXPECTED_COLUMNS = [\"EmployeeID\", \"Name\", \"BasicPay\", \"Allowances\", \"Deductions\"]\n",
        "\n",
        "    def parse(self, file_path: str, tenant_id: str) -> Dict[str, Any]:\n",
        "        logger = setup_logging(tenant_id)\n",
        "        try:\n",
        "            df = pd.read_excel(file_path)\n",
        "            if not all(col in df.columns for col in self.EXPECTED_COLUMNS):\n",
        "                raise LedgerOneError(\"XLSX missing required columns\")\n",
        "\n",
        "            records = []\n",
        "            for _, row in df.iterrows():\n",
        "                record = {\n",
        "                    \"doc_id\": f\"xlsx_{hashlib.md5(str(row).encode()).hexdigest()}\",\n",
        "                    \"tenant_id\": tenant_id,\n",
        "                    \"employee_id\": row[\"EmployeeID\"],\n",
        "                    \"name\": row[\"Name\"],\n",
        "                    \"basic_pay\": float(row[\"BasicPay\"]),\n",
        "                    \"allowances\": float(row[\"Allowances\"] or 0),\n",
        "                    \"deductions\": float(row[\"Deductions\"] or 0),\n",
        "                    \"source_type\": \"payroll\",\n",
        "                }\n",
        "                records.append(record)\n",
        "\n",
        "            logger.info(f\"Parsed {len(records)} payroll records\", extra={\"action\": \"parse_xlsx\"})\n",
        "            return {\"records\": records, \"metadata\": {\"parser\": \"xlsx\", \"status\": \"success\"}}\n",
        "        except Exception as e:\n",
        "            logger.error(f\"XLSX parse failed: {str(e)}\", extra={\"action\": \"parse_xlsx\"})\n",
        "            raise LedgerOneError(f\"XLSX parse failed: {str(e)}\")\n",
        "\n",
        "# PDF/Image Parser (Invoices, fallback to Tesseract)\n",
        "class PDFImageParser(Parser):\n",
        "    \"\"\"Parses PDF/image invoices using Tesseract as fallback.\"\"\"\n",
        "    def parse(self, file_path: str, tenant_id: str) -> Dict[str, Any]:\n",
        "        logger = setup_logging(tenant_id)\n",
        "        try:\n",
        "            # Convert PDF to images\n",
        "            images = convert_from_path(file_path) if file_path.endswith(\".pdf\") else [Image.open(file_path)]\n",
        "            text = \"\"\n",
        "            for img in images:\n",
        "                text += pytesseract.image_to_string(img)\n",
        "\n",
        "            # Extract invoice fields (basic regex, improved in cell_04)\n",
        "            invoice_data = self._extract_invoice_fields(text, tenant_id)\n",
        "            logger.info(\"Parsed PDF/image invoice\", extra={\"action\": \"parse_pdf_image\"})\n",
        "            return {\n",
        "                \"records\": [invoice_data],\n",
        "                \"metadata\": {\"parser\": \"tesseract\", \"status\": \"success\"},\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"PDF/image parse failed: {str(e)}\", extra={\"action\": \"parse_pdf_image\"})\n",
        "            raise LedgerOneError(f\"PDF/image parse failed: {str(e)}\")\n",
        "\n",
        "    def _extract_invoice_fields(self, text: str, tenant_id: str) -> Dict[str, Any]:\n",
        "        \"\"\"Extract invoice fields using regex (stub, enhanced in cell_04).\"\"\"\n",
        "        invoice_number = re.search(r\"Invoice\\s*#?\\s*(\\w+)\", text, re.IGNORECASE)\n",
        "        total = re.search(r\"Total\\s*[:=]\\s*(\\d+\\.?\\d*)\", text, re.IGNORECASE)\n",
        "        return {\n",
        "            \"doc_id\": f\"pdf_{hashlib.md5(text.encode()).hexdigest()}\",\n",
        "            \"tenant_id\": tenant_id,\n",
        "            \"invoice_number\": invoice_number.group(1) if invoice_number else \"unknown\",\n",
        "            \"total\": float(total.group(1)) if total else 0.0,\n",
        "            \"currency\": \"KES\",\n",
        "            \"source_type\": \"invoice\",\n",
        "        }\n",
        "\n",
        "# Ingestion service\n",
        "class IngestionService:\n",
        "    \"\"\"Handles file ingestion and normalization.\"\"\"\n",
        "    def __init__(self, tenant_id: str):\n",
        "        self.tenant_id = tenant_id\n",
        "        self.parsers = {\n",
        "            \".csv\": CSVParser(),\n",
        "            \".xlsx\": XLSXParser(),\n",
        "            \".pdf\": PDFImageParser(),\n",
        "            \".png\": PDFImageParser(),\n",
        "            \".jpg\": PDFImageParser(),\n",
        "        }\n",
        "\n",
        "    def ingest_file(self, file_path: str) -> DocumentMetadata:\n",
        "        \"\"\"Ingest a file and store raw/normalized data.\"\"\"\n",
        "        logger = setup_logging(self.tenant_id)\n",
        "        file_ext = Path(file_path).suffix.lower()\n",
        "        parser = self.parsers.get(file_ext)\n",
        "        if not parser:\n",
        "            logger.error(f\"Unsupported file type: {file_ext}\", extra={\"action\": \"ingest_file\"})\n",
        "            raise LedgerOneError(f\"Unsupported file type: {file_ext}\")\n",
        "\n",
        "        # Store raw file\n",
        "        raw_path = UPLOAD_DIR / self.tenant_id / Path(file_path).name\n",
        "        raw_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        with open(raw_path, \"wb\") as f:\n",
        "            with open(file_path, \"rb\") as src:\n",
        "                f.write(src.read())\n",
        "\n",
        "        # Parse and normalize\n",
        "        parsed_data = parser.parse(file_path, self.tenant_id)\n",
        "        doc_id = parsed_data[\"records\"][0][\"doc_id\"]\n",
        "        normalized_path = NORMALIZED_DIR / self.tenant_id / f\"{doc_id}.json\"\n",
        "        normalized_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        with open(normalized_path, \"w\") as f:\n",
        "            json.dump(parsed_data[\"records\"], f, indent=2)\n",
        "\n",
        "        metadata = DocumentMetadata(\n",
        "            doc_id=doc_id,\n",
        "            tenant_id=self.tenant_id,\n",
        "            source_type=file_ext[1:],\n",
        "            file_path=str(raw_path),\n",
        "            ingested_at=datetime.utcnow().isoformat(),\n",
        "            parser_used=parsed_data[\"metadata\"][\"parser\"],\n",
        "            status=parsed_data[\"metadata\"][\"status\"],\n",
        "        )\n",
        "        logger.info(f\"Ingested file: {file_path}\", extra={\"action\": \"ingest_file\", \"doc_id\": doc_id})\n",
        "        return metadata\n",
        "\n",
        "# FastAPI endpoints\n",
        "from fastapi import UploadFile, File\n",
        "@app.post(\"/ingest\")\n",
        "async def ingest_file(file: UploadFile = File(...), tenant_id: str = Depends(get_current_user)):\n",
        "    \"\"\"Ingest uploaded file.\"\"\"\n",
        "    try:\n",
        "        service = IngestionService(tenant_id=tenant_id[\"tenant_id\"])\n",
        "        file_path = UPLOAD_DIR / tenant_id[\"tenant_id\"] / file.filename\n",
        "        with open(file_path, \"wb\") as f:\n",
        "            f.write(await file.read())\n",
        "        metadata = service.ingest_file(str(file_path))\n",
        "        return metadata.dict()\n",
        "    except LedgerOneError as e:\n",
        "        raise HTTPException(status_code=400, detail=str(e))\n",
        "\n",
        "# Unit tests\n",
        "def test_csv_parser(tmp_path):\n",
        "    \"\"\"Test CSV parser for bank statements.\"\"\"\n",
        "    csv_content = \"Date,Description,Debit,Credit,Balance\\n2025-09-01,Payment,1000,,5000\"\n",
        "    file_path = tmp_path / \"test.csv\"\n",
        "    file_path.write_text(csv_content)\n",
        "    parser = CSVParser()\n",
        "    result = parser.parse(str(file_path), \"test_tenant\")\n",
        "    assert len(result[\"records\"]) == 1\n",
        "    assert result[\"records\"][0][\"amount\"] == -1000.0\n",
        "    assert result[\"metadata\"][\"status\"] == \"success\"\n",
        "\n",
        "def test_xlsx_parser(tmp_path):\n",
        "    \"\"\"Test XLSX parser for payroll.\"\"\"\n",
        "    df = pd.DataFrame({\n",
        "        \"EmployeeID\": [\"E001\"],\n",
        "        \"Name\": [\"John Doe\"],\n",
        "        \"BasicPay\": [50000],\n",
        "        \"Allowances\": [5000],\n",
        "        \"Deductions\": [2000],\n",
        "    })\n",
        "    file_path = tmp_path / \"test.xlsx\"\n",
        "    df.to_excel(file_path, index=False)\n",
        "    parser = XLSXParser()\n",
        "    result = parser.parse(str(file_path), \"test_tenant\")\n",
        "    assert len(result[\"records\"]) == 1\n",
        "    assert result[\"records\"][0][\"basic_pay\"] == 50000.0\n",
        "\n",
        "def test_ingestion_service(tmp_path):\n",
        "    \"\"\"Test ingestion service end-to-end.\"\"\"\n",
        "    csv_content = \"Date,Description,Debit,Credit,Balance\\n2025-09-01,Payment,1000,,5000\"\n",
        "    file_path = tmp_path / \"test.csv\"\n",
        "    file_path.write_text(csv_content)\n",
        "    service = IngestionService(tenant_id=\"test_tenant\")\n",
        "    metadata = service.ingest_file(str(file_path))\n",
        "    assert metadata.doc_id\n",
        "    assert (NORMALIZED_DIR / \"test_tenant\" / f\"{metadata.doc_id}.json\").exists()\n",
        "\n",
        "# Smoke test\n",
        "def run_smoke_test():\n",
        "    \"\"\"Run ingestion smoke tests.\"\"\"\n",
        "    import tempfile\n",
        "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
        "        test_csv_parser(Path(tmpdirname))\n",
        "        test_xlsx_parser(Path(tmpdirname))\n",
        "        test_ingestion_service(Path(tmpdirname))\n",
        "    print(\"Ingestion smoke tests passed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_smoke_test()"
      ],
      "metadata": {
        "id": "gkXa2lwtUmqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cell_03_canonical_schema_featurestore.py\n",
        "\"\"\"\n",
        "LedgerOne Canonical Schema and Feature Store\n",
        "==========================================\n",
        "Defines the canonical schema for documents (invoices, bank statements, payroll) using Pydantic\n",
        "and implements a JSON-based feature store for AI/ML preprocessing. Stores data in\n",
        "data/canonical/{tenant}/ as plaintext JSON/CSV. Provides migration helpers for future DB.\n",
        "Key features:\n",
        "- CanonicalDocument model with strict validation (Kenyan-specific fields: VAT, M-Pesa).\n",
        "- Feature store for TF-IDF, numeric features, and embeddings.\n",
        "- Migration helpers for JSON-to-DB transition.\n",
        "- Unit tests for schema validation and feature extraction.\n",
        "\n",
        "Configuration:\n",
        "- Uses constants from cell_01_core_bootstrap.py.\n",
        "- Feature store data in data/features/{tenant}/.\n",
        "\n",
        "Extension points:\n",
        "- Add new schema fields in CanonicalDocument.\n",
        "- Extend feature extractors in FeatureStore.\n",
        "- Implement DB migrations in migrate_to_db().\n",
        "\"\"\"\n",
        "import json\n",
        "import csv\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Optional, Any, Union\n",
        "from datetime import datetime\n",
        "import logging\n",
        "import hashlib\n",
        "from pydantic import BaseModel, Field, validator, ValidationError\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from ledgerone_prototype.cell_01_core_bootstrap import setup_logging, LedgerOneError, TenantContext, config, KENYA_CONSTANTS\n",
        "\n",
        "# File paths\n",
        "FEATURES_DIR = Path(config.DATA_DIR) / \"features\"\n",
        "CANONICAL_DIR = Path(config.CANONICAL_DIR)\n",
        "FEATURES_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Canonical schema models\n",
        "class Vendor(BaseModel):\n",
        "    raw: str\n",
        "    normalized: Optional[str] = None\n",
        "    vendor_id: Optional[str] = None\n",
        "\n",
        "class LineItem(BaseModel):\n",
        "    description: str\n",
        "    quantity: float\n",
        "    unit_price: float\n",
        "    total: float\n",
        "    vat_rate: float = KENYA_CONSTANTS[\"VAT_RATE\"]  # Default 16%\n",
        "\n",
        "class VatLine(BaseModel):\n",
        "    rate: float = KENYA_CONSTANTS[\"VAT_RATE\"]\n",
        "    amount: float\n",
        "\n",
        "class CanonicalDocument(BaseModel):\n",
        "    doc_id: str\n",
        "    tenant_id: str\n",
        "    source_type: str  # invoice, bank_statement, payroll\n",
        "    vendor: Optional[Vendor] = None\n",
        "    lines: List[LineItem] = []\n",
        "    total: float\n",
        "    vat_lines: List[VatLine] = []\n",
        "    currency: str = \"KES\"\n",
        "    dates: Dict[str, str] = {}  # e.g., {\"issue\": \"2025-09-10\", \"due\": \"2025-10-10\"}\n",
        "    parsed_by: str  # csv, xlsx, tesseract\n",
        "    confidence: float = Field(ge=0.0, le=1.0)\n",
        "    features: Dict[str, Any] = {}  # ML features (e.g., TF-IDF, embeddings)\n",
        "    mpesa_transaction_id: Optional[str] = None  # Kenyan-specific\n",
        "\n",
        "    @validator(\"doc_id\")\n",
        "    def validate_doc_id(cls, v):\n",
        "        if not v:\n",
        "            raise ValueError(\"doc_id cannot be empty\")\n",
        "        return v\n",
        "\n",
        "    @validator(\"source_type\")\n",
        "    def validate_source_type(cls, v):\n",
        "        valid_types = [\"invoice\", \"bank_statement\", \"payroll\"]\n",
        "        if v not in valid_types:\n",
        "            raise ValueError(f\"Invalid source_type: {v}\")\n",
        "        return v\n",
        "\n",
        "    @validator(\"vat_lines\")\n",
        "    def validate_vat_lines(cls, v, values):\n",
        "        if \"total\" in values and v:\n",
        "            vat_total = sum(vl.amount for vl in v)\n",
        "            if abs(vat_total - values[\"total\"] * KENYA_CONSTANTS[\"VAT_RATE\"]) > 0.01:\n",
        "                logging.warning(\"VAT total mismatch\", extra={\"doc_id\": values.get(\"doc_id\")})\n",
        "        return v\n",
        "\n",
        "# Feature store\n",
        "class FeatureStore:\n",
        "    \"\"\"Manages ML features for documents.\"\"\"\n",
        "    def __init__(self, tenant_id: str):\n",
        "        self.tenant_id = tenant_id\n",
        "        self.vectorizer = TfidfVectorizer(max_features=1000)\n",
        "        self.feature_dir = FEATURES_DIR / tenant_id\n",
        "        self.feature_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.logger = setup_logging(tenant_id)\n",
        "\n",
        "    def extract_features(self, doc: CanonicalDocument) -> Dict[str, Any]:\n",
        "        \"\"\"Extract ML features (TF-IDF, numeric).\"\"\"\n",
        "        try:\n",
        "            # Text features (e.g., vendor, description)\n",
        "            text = (doc.vendor.raw if doc.vendor else \"\") + \" \" + \" \".join(\n",
        "                li.description for li in doc.lines\n",
        "            )\n",
        "            tfidf = self.vectorizer.fit_transform([text]).toarray()[0].tolist()\n",
        "            features = {\n",
        "                \"tfidf\": tfidf,\n",
        "                \"total\": doc.total,\n",
        "                \"line_count\": len(doc.lines),\n",
        "                \"vat_amount\": sum(vl.amount for vl in doc.vat_lines),\n",
        "            }\n",
        "            self._save_features(doc.doc_id, features)\n",
        "            self.logger.info(f\"Extracted features for {doc.doc_id}\", extra={\"action\": \"extract_features\"})\n",
        "            return features\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Feature extraction failed: {str(e)}\", extra={\"action\": \"extract_features\"})\n",
        "            raise LedgerOneError(f\"Feature extraction failed: {str(e)}\")\n",
        "\n",
        "    def _save_features(self, doc_id: str, features: Dict[str, Any]):\n",
        "        \"\"\"Save features to JSON.\"\"\"\n",
        "        with open(self.feature_dir / f\"{doc_id}.json\", \"w\") as f:\n",
        "            json.dump(features, f, indent=2)\n",
        "\n",
        "    def load_features(self, doc_id: str) -> Dict[str, Any]:\n",
        "        \"\"\"Load features from JSON.\"\"\"\n",
        "        try:\n",
        "            with open(self.feature_dir / f\"{doc_id}.json\") as f:\n",
        "                return json.load(f)\n",
        "        except FileNotFoundError:\n",
        "            self.logger.error(f\"Features not found: {doc_id}\", extra={\"action\": \"load_features\"})\n",
        "            raise LedgerOneError(f\"Features not found: {doc_id}\")\n",
        "\n",
        "# Normalization service\n",
        "class NormalizationService:\n",
        "    \"\"\"Converts ingested data to canonical schema.\"\"\"\n",
        "    def __init__(self, tenant_id: str):\n",
        "        self.tenant_id = tenant_id\n",
        "        self.feature_store = FeatureStore(tenant_id)\n",
        "        self.logger = setup_logging(tenant_id)\n",
        "\n",
        "    def normalize(self, parsed_data: Dict[str, Any], parser: str) -> CanonicalDocument:\n",
        "        \"\"\"Normalize parsed data to canonical schema.\"\"\"\n",
        "        try:\n",
        "            source_type = parsed_data[\"records\"][0][\"source_type\"]\n",
        "            doc_id = parsed_data[\"records\"][0][\"doc_id\"]\n",
        "            if source_type == \"invoice\":\n",
        "                doc = self._normalize_invoice(parsed_data, parser)\n",
        "            elif source_type == \"bank_statement\":\n",
        "                doc = self._normalize_bank_statement(parsed_data, parser)\n",
        "            elif source_type == \"payroll\":\n",
        "                doc = self._normalize_payroll(parsed_data, parser)\n",
        "            else:\n",
        "                raise LedgerOneError(f\"Unknown source_type: {source_type}\")\n",
        "\n",
        "            # Extract and attach features\n",
        "            doc.features = self.feature_store.extract_features(doc)\n",
        "            self._save_canonical(doc)\n",
        "            self.logger.info(f\"Normalized doc: {doc_id}\", extra={\"action\": \"normalize\"})\n",
        "            return doc\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Normalization failed: {str(e)}\", extra={\"action\": \"normalize\"})\n",
        "            raise LedgerOneError(f\"Normalization failed: {str(e)}\")\n",
        "\n",
        "    def _normalize_invoice(self, parsed_data: Dict[str, Any], parser: str) -> CanonicalDocument:\n",
        "        \"\"\"Normalize invoice data.\"\"\"\n",
        "        record = parsed_data[\"records\"][0]\n",
        "        return CanonicalDocument(\n",
        "            doc_id=record[\"doc_id\"],\n",
        "            tenant_id=self.tenant_id,\n",
        "            source_type=\"invoice\",\n",
        "            vendor=Vendor(raw=record.get(\"invoice_number\", \"unknown\")),\n",
        "            lines=[LineItem(description=\"Item\", quantity=1, unit_price=record[\"total\"], total=record[\"total\"])],\n",
        "            total=record[\"total\"],\n",
        "            vat_lines=[VatLine(amount=record[\"total\"] * KENYA_CONSTANTS[\"VAT_RATE\"])],\n",
        "            currency=\"KES\",\n",
        "            dates={\"issue\": datetime.utcnow().isoformat()},\n",
        "            parsed_by=parser,\n",
        "            confidence=0.8,  # Stub, refined in cell_04\n",
        "        )\n",
        "\n",
        "    def _normalize_bank_statement(self, parsed_data: Dict[str, Any], parser: str) -> CanonicalDocument:\n",
        "        \"\"\"Normalize bank statement data.\"\"\"\n",
        "        record = parsed_data[\"records\"][0]\n",
        "        return CanonicalDocument(\n",
        "            doc_id=record[\"doc_id\"],\n",
        "            tenant_id=self.tenant_id,\n",
        "            source_type=\"bank_statement\",\n",
        "            vendor=Vendor(raw=record[\"description\"]),\n",
        "            lines=[],\n",
        "            total=record[\"amount\"],\n",
        "            vat_lines=[],\n",
        "            currency=\"KES\",\n",
        "            dates={\"transaction\": record[\"date\"]},\n",
        "            parsed_by=parser,\n",
        "            confidence=0.9,\n",
        "            mpesa_transaction_id=record.get(\"description\", \"\").split()[-1] if \"MPESA\" in record[\"description\"] else None,\n",
        "        )\n",
        "\n",
        "    def _normalize_payroll(self, parsed_data: Dict[str, Any], parser: str) -> CanonicalDocument:\n",
        "        \"\"\"Normalize payroll data.\"\"\"\n",
        "        record = parsed_data[\"records\"][0]\n",
        "        return CanonicalDocument(\n",
        "            doc_id=record[\"doc_id\"],\n",
        "            tenant_id=self.tenant_id,\n",
        "            source_type=\"payroll\",\n",
        "            vendor=Vendor(raw=record[\"name\"]),\n",
        "            lines=[LineItem(description=\"Salary\", quantity=1, unit_price=record[\"basic_pay\"], total=record[\"basic_pay\"])],\n",
        "            total=record[\"basic_pay\"] + record[\"allowances\"] - record[\"deductions\"],\n",
        "            vat_lines=[],\n",
        "            currency=\"KES\",\n",
        "            dates={\"payroll_date\": datetime.utcnow().isoformat()},\n",
        "            parsed_by=parser,\n",
        "            confidence=0.85,\n",
        "        )\n",
        "\n",
        "    def _save_canonical(self, doc: CanonicalDocument):\n",
        "        \"\"\"Save canonical document as JSON.\"\"\"\n",
        "        path = CANONICAL_DIR / self.tenant_id / f\"{doc.doc_id}.json\"\n",
        "        path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        with open(path, \"w\") as f:\n",
        "            json.dump(doc.dict(), f, indent=2)\n",
        "\n",
        "# Migration helper (stub for DB)\n",
        "def migrate_to_db(tenant_id: str, canonical_dir: Path = CANONICAL_DIR):\n",
        "    \"\"\"Stub for migrating JSON to SQLAlchemy DB.\"\"\"\n",
        "    logger = setup_logging(tenant_id)\n",
        "    logger.info(\"Starting JSON-to-DB migration\", extra={\"action\": \"migrate_to_db\"})\n",
        "    # TODO: Implement SQLAlchemy migration in production\n",
        "    for json_file in (CANONICAL_DIR / tenant_id).glob(\"*.json\"):\n",
        "        with open(json_file) as f:\n",
        "            data = json.load(f)\n",
        "            logger.info(f\"Migration stub processed: {json_file.name}\", extra={\"action\": \"migrate_to_db\"})\n",
        "    logger.info(\"Migration stub complete\", extra={\"action\": \"migrate_to_db\"})\n",
        "\n",
        "# FastAPI endpoints\n",
        "from fastapi import HTTPException\n",
        "@app.post(\"/normalize\")\n",
        "async def normalize_document(parsed_data: Dict[str, Any], tenant_id: str = Depends(get_current_user)):\n",
        "    \"\"\"Normalize parsed data to canonical schema.\"\"\"\n",
        "    try:\n",
        "        service = NormalizationService(tenant_id=tenant_id[\"tenant_id\"])\n",
        "        doc = service.normalize(parsed_data, parser=parsed_data[\"metadata\"][\"parser\"])\n",
        "        return doc.dict()\n",
        "    except LedgerOneError as e:\n",
        "        raise HTTPException(status_code=400, detail=str(e))\n",
        "\n",
        "# Unit tests\n",
        "def test_canonical_document_validation():\n",
        "    \"\"\"Test CanonicalDocument validation.\"\"\"\n",
        "    doc = CanonicalDocument(\n",
        "        doc_id=\"test_001\",\n",
        "        tenant_id=\"test_tenant\",\n",
        "        source_type=\"invoice\",\n",
        "        total=1000.0,\n",
        "        vat_lines=[VatLine(amount=160.0)],\n",
        "        parsed_by=\"csv\",\n",
        "        confidence=0.8,\n",
        "    )\n",
        "    assert doc.total == 1000.0\n",
        "    with pytest.raises(ValidationError):\n",
        "        CanonicalDocument(\n",
        "            doc_id=\"\",\n",
        "            tenant_id=\"test_tenant\",\n",
        "            source_type=\"invalid\",\n",
        "            total=1000.0,\n",
        "            parsed_by=\"csv\",\n",
        "            confidence=0.8,\n",
        "        )\n",
        "\n",
        "def test_feature_store(tmp_path):\n",
        "    \"\"\"Test feature store operations.\"\"\"\n",
        "    feature_dir = tmp_path / \"features\" / \"test_tenant\"\n",
        "    feature_dir.mkdir(parents=True)\n",
        "    store = FeatureStore(tenant_id=\"test_tenant\")\n",
        "    doc = CanonicalDocument(\n",
        "        doc_id=\"test_001\",\n",
        "        tenant_id=\"test_tenant\",\n",
        "        source_type=\"invoice\",\n",
        "        vendor=Vendor(raw=\"Test Vendor\"),\n",
        "        lines=[LineItem(description=\"Item 1\", quantity=1, unit_price=1000, total=1000)],\n",
        "        total=1000.0,\n",
        "        parsed_by=\"csv\",\n",
        "        confidence=0.8,\n",
        "    )\n",
        "    features = store.extract_features(doc)\n",
        "    assert \"tfidf\" in features\n",
        "    assert features[\"total\"] == 1000.0\n",
        "    assert (feature_dir / \"test_001.json\").exists()\n",
        "\n",
        "def test_normalization_service(tmp_path):\n",
        "    \"\"\"Test normalization service.\"\"\"\n",
        "    parsed_data = {\n",
        "        \"records\": [{\n",
        "            \"doc_id\": \"test_001\",\n",
        "            \"tenant_id\": \"test_tenant\",\n",
        "            \"source_type\": \"invoice\",\n",
        "            \"invoice_number\": \"INV001\",\n",
        "            \"total\": 1000.0,\n",
        "        }],\n",
        "        \"metadata\": {\"parser\": \"csv\", \"status\": \"success\"},\n",
        "    }\n",
        "    service = NormalizationService(tenant_id=\"test_tenant\")\n",
        "    doc = service.normalize(parsed_data, parser=\"csv\")\n",
        "    assert doc.doc_id == \"test_001\"\n",
        "    assert doc.source_type == \"invoice\"\n",
        "    assert (CANONICAL_DIR / \"test_tenant\" / \"test_001.json\").exists()\n",
        "\n",
        "# Smoke test\n",
        "def run_smoke_test():\n",
        "    \"\"\"Run canonical schema smoke tests.\"\"\"\n",
        "    import tempfile\n",
        "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
        "        global CANONICAL_DIR, FEATURES_DIR\n",
        "        CANONICAL_DIR = Path(tmpdirname) / \"canonical\"\n",
        "        FEATURES_DIR = Path(tmpdirname) / \"features\"\n",
        "        test_canonical_document_validation()\n",
        "        test_feature_store(Path(tmpdirname))\n",
        "        test_normalization_service(Path(tmpdirname))\n",
        "    print(\"Canonical schema smoke tests passed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_smoke_test()"
      ],
      "metadata": {
        "id": "d5oCTCyZUynK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cell_04_ai_services.py\n",
        "\"\"\"\n",
        "LedgerOne AI Services\n",
        "=====================\n",
        "Implements AI-driven document processing: vendor normalization, category classification,\n",
        "and anomaly detection. Supports Kenyan-specific heuristics (e.g., vendor names, invoice types).\n",
        "Includes baseline ML models and optional LLM integration. Logs corrections for retraining.\n",
        "Key features:\n",
        "- Vendor normalization: Fuzzy matching + TF-IDF.\n",
        "- Category classifier: LightGBM with heuristic features.\n",
        "- Anomaly detection: IsolationForest on numeric features.\n",
        "- Optional LLM support (OpenAI/Claude via env var).\n",
        "- Correction logging to data/ml_corrections/{tenant}/.\n",
        "- Unit tests for model predictions.\n",
        "\n",
        "Configuration:\n",
        "- Set LLM_API_KEY in .env for LLM support (falls back to template-based assistant).\n",
        "- Models stored in data/models/{tenant}/.\n",
        "\n",
        "Extension points:\n",
        "- Add new models in ModelRegistry.\n",
        "- Extend feature pipelines in FeatureExtractor.\n",
        "\"\"\"\n",
        "import json\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Optional, Any\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "from fuzzywuzzy import fuzz\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import lightgbm as lgb\n",
        "from pydantic import BaseModel\n",
        "import requests\n",
        "from ledgerone_prototype.cell_01_core_bootstrap import setup_logging, LedgerOneError, config, TenantContext\n",
        "from ledgerone_prototype.cell_03_canonical_schema_featurestore import CanonicalDocument, FeatureStore\n",
        "\n",
        "# File paths\n",
        "MODELS_DIR = Path(config.DATA_DIR) / \"models\"\n",
        "CORRECTIONS_DIR = Path(config.DATA_DIR) / \"ml_corrections\"\n",
        "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "CORRECTIONS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Model registry\n",
        "class ModelRegistry(BaseModel):\n",
        "    model_type: str  # vendor, category, anomaly\n",
        "    model_path: str\n",
        "    version: str\n",
        "    trained_at: str\n",
        "\n",
        "class AIService:\n",
        "    \"\"\"Manages AI models and predictions.\"\"\"\n",
        "    def __init__(self, tenant_id: str):\n",
        "        self.tenant_id = tenant_id\n",
        "        self.logger = setup_logging(tenant_id)\n",
        "        self.feature_store = FeatureStore(tenant_id)\n",
        "        self.vectorizer = TfidfVectorizer(max_features=1000)\n",
        "        self.category_model = lgb.LGBMClassifier()\n",
        "        self.anomaly_model = IsolationForest(contamination=0.1)\n",
        "        self.known_vendors = [\"Safaricom\", \"KPLC\", \"Nairobi Water\"]  # Kenyan-specific\n",
        "        self.categories = [\"Utilities\", \"Supplies\", \"Services\"]  # KNBS-derived\n",
        "        self.model_registry = self._load_registry()\n",
        "        self._initialize_models()\n",
        "\n",
        "    def _load_registry(self) -> List[ModelRegistry]:\n",
        "        \"\"\"Load model registry from JSON.\"\"\"\n",
        "        registry_path = MODELS_DIR / self.tenant_id / \"registry.json\"\n",
        "        if registry_path.exists():\n",
        "            with open(registry_path) as f:\n",
        "                return [ModelRegistry(**m) for m in json.load(f)]\n",
        "        return []\n",
        "\n",
        "    def _initialize_models(self):\n",
        "        \"\"\"Initialize or load models.\"\"\"\n",
        "        try:\n",
        "            # Stub: Train simple models with heuristic data\n",
        "            X = self.vectorizer.fit_transform(self.known_vendors + self.categories).toarray()\n",
        "            y = [1] * len(self.known_vendors) + [0] * len(self.categories)  # Dummy labels\n",
        "            self.category_model.fit(X, y[:len(self.categories)])\n",
        "            self.anomaly_model.fit(X)\n",
        "            self._save_registry()\n",
        "            self.logger.info(\"Models initialized\", extra={\"action\": \"initialize_models\"})\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Model init failed: {str(e)}\", extra={\"action\": \"initialize_models\"})\n",
        "            raise LedgerOneError(f\"Model init failed: {str(e)}\")\n",
        "\n",
        "    def _save_registry(self):\n",
        "        \"\"\"Save model registry.\"\"\"\n",
        "        registry = [\n",
        "            ModelRegistry(\n",
        "                model_type=\"category\",\n",
        "                model_path=str(MODELS_DIR / self.tenant_id / \"category.model\"),\n",
        "                version=\"1.0\",\n",
        "                trained_at=datetime.utcnow().isoformat(),\n",
        "            ),\n",
        "            ModelRegistry(\n",
        "                model_type=\"anomaly\",\n",
        "                model_path=str(MODELS_DIR / self.tenant_id / \"anomaly.model\"),\n",
        "                version=\"1.0\",\n",
        "                trained_at=datetime.utcnow().isoformat(),\n",
        "            ),\n",
        "        ]\n",
        "        with open(MODELS_DIR / self.tenant_id / \"registry.json\", \"w\") as f:\n",
        "            json.dump([r.dict() for r in registry], f, indent=2)\n",
        "\n",
        "    def normalize_vendor(self, doc: CanonicalDocument) -> str:\n",
        "        \"\"\"Normalize vendor name using fuzzy matching.\"\"\"\n",
        "        try:\n",
        "            raw_vendor = doc.vendor.raw if doc.vendor else \"\"\n",
        "            if not raw_vendor:\n",
        "                return \"Unknown\"\n",
        "            scores = [fuzz.ratio(raw_vendor.lower(), v.lower()) for v in self.known_vendors]\n",
        "            best_idx = np.argmax(scores)\n",
        "            if scores[best_idx] > 80:\n",
        "                return self.known_vendors[best_idx]\n",
        "            return raw_vendor  # Fallback to raw\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Vendor normalization failed: {str(e)}\", extra={\"action\": \"normalize_vendor\"})\n",
        "            raise LedgerOneError(f\"Vendor normalization failed: {str(e)}\")\n",
        "\n",
        "    def predict_category(self, doc: CanonicalDocument) -> Dict[str, float]:\n",
        "        \"\"\"Predict document category with confidence.\"\"\"\n",
        "        try:\n",
        "            features = self.feature_store.load_features(doc.doc_id)\n",
        "            X = np.array([features[\"tfidf\"]])\n",
        "            probs = self.category_model.predict_proba(X)[0]\n",
        "            return {cat: float(prob) for cat, prob in zip(self.categories, probs)}\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Category prediction failed: {str(e)}\", extra={\"action\": \"predict_category\"})\n",
        "            return {cat: 1.0 / len(self.categories) for cat in self.categories}  # Uniform fallback\n",
        "\n",
        "    def detect_anomaly(self, doc: CanonicalDocument) -> bool:\n",
        "        \"\"\"Detect anomalies in document.\"\"\"\n",
        "        try:\n",
        "            features = self.feature_store.load_features(doc.doc_id)\n",
        "            X = np.array([[features[\"total\"], features[\"line_count\"], features[\"vat_amount\"]]])\n",
        "            return bool(self.anomaly_model.predict(X)[0] == -1)\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Anomaly detection failed: {str(e)}\", extra={\"action\": \"detect_anomaly\"})\n",
        "            return False\n",
        "\n",
        "    def llm_suggestion(self, doc: CanonicalDocument) -> Optional[str]:\n",
        "        \"\"\"Optional LLM-based suggestion (if API key provided).\"\"\"\n",
        "        if not config.LLM_API_KEY:\n",
        "            self.logger.info(\"No LLM_API_KEY; using template\", extra={\"action\": \"llm_suggestion\"})\n",
        "            return f\"Categorized as {self.predict_category(doc).max()} (template-based)\"\n",
        "        try:\n",
        "            response = requests.post(\n",
        "                \"https://api.openai.com/v1/completions\",\n",
        "                headers={\"Authorization\": f\"Bearer {config.LLM_API_KEY}\"},\n",
        "                json={\"prompt\": f\"Categorize document: {doc.dict()}\", \"model\": \"text-davinci-003\"},\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            return response.json()[\"choices\"][0][\"text\"]\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"LLM suggestion failed: {str(e)}\", extra={\"action\": \"llm_suggestion\"})\n",
        "            return None\n",
        "\n",
        "    def log_correction(self, doc_id: str, correction: Dict[str, Any]):\n",
        "        \"\"\"Log user corrections for retraining.\"\"\"\n",
        "        correction_path = CORRECTIONS_DIR / self.tenant_id / f\"{doc_id}.json\"\n",
        "        correction_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        with open(correction_path, \"w\") as f:\n",
        "            json.dump(correction, f, indent=2)\n",
        "        self.logger.info(f\"Logged correction for {doc_id}\", extra={\"action\": \"log_correction\"})\n",
        "\n",
        "# FastAPI endpoints\n",
        "from fastapi import HTTPException\n",
        "@app.post(\"/ai/predict\")\n",
        "async def predict(doc: CanonicalDocument, tenant_id: str = Depends(get_current_user)):\n",
        "    \"\"\"Generate AI predictions for document.\"\"\"\n",
        "    try:\n",
        "        service = AIService(tenant_id=tenant_id[\"tenant_id\"])\n",
        "        predictions = {\n",
        "            \"vendor\": service.normalize_vendor(doc),\n",
        "            \"category\": service.predict_category(doc),\n",
        "            \"anomaly\": service.detect_anomaly(doc),\n",
        "            \"llm_suggestion\": service.llm_suggestion(doc),\n",
        "        }\n",
        "        return predictions\n",
        "    except LedgerOneError as e:\n",
        "        raise HTTPException(status_code=400, detail=str(e))\n",
        "\n",
        "@app.post(\"/ai/correct\")\n",
        "async def log_correction_endpoint(doc_id: str, correction: Dict[str, Any], tenant_id: str = Depends(get_current_user)):\n",
        "    \"\"\"Log correction for retraining.\"\"\"\n",
        "    try:\n",
        "        service = AIService(tenant_id=tenant_id[\"tenant_id\"])\n",
        "        service.log_correction(doc_id, correction)\n",
        "        return {\"status\": \"correction logged\"}\n",
        "    except LedgerOneError as e:\n",
        "        raise HTTPException(status_code=400, detail=str(e))\n",
        "\n",
        "# Unit tests\n",
        "def test_vendor_normalization():\n",
        "    \"\"\"Test vendor normalization.\"\"\"\n",
        "    service = AIService(tenant_id=\"test_tenant\")\n",
        "    doc = CanonicalDocument(\n",
        "        doc_id=\"test_001\",\n",
        "        tenant_id=\"test_tenant\",\n",
        "        source_type=\"invoice\",\n",
        "        vendor=Vendor(raw=\"Safaricom Ltd\"),\n",
        "        total=1000.0,\n",
        "        parsed_by=\"csv\",\n",
        "        confidence=0.8,\n",
        "    )\n",
        "    assert service.normalize_vendor(doc) == \"Safaricom\"\n",
        "\n",
        "def test_category_prediction():\n",
        "    \"\"\"Test category prediction.\"\"\"\n",
        "    service = AIService(tenant_id=\"test_tenant\")\n",
        "    doc = CanonicalDocument(\n",
        "        doc_id=\"test_001\",\n",
        "        tenant_id=\"test_tenant\",\n",
        "        source_type=\"invoice\",\n",
        "        vendor=Vendor(raw=\"KPLC\"),\n",
        "        total=1000.0,\n",
        "        parsed_by=\"csv\",\n",
        "        confidence=0.8,\n",
        "    )\n",
        "    service.feature_store.extract_features(doc)\n",
        "    categories = service.predict_category(doc)\n",
        "    assert \"Utilities\" in categories\n",
        "    assert sum(categories.values()) <= 1.0\n",
        "\n",
        "def test_anomaly_detection():\n",
        "    \"\"\"Test anomaly detection.\"\"\"\n",
        "    service = AIService(tenant_id=\"test_tenant\")\n",
        "    doc = CanonicalDocument(\n",
        "        doc_id=\"test_001\",\n",
        "        tenant_id=\"test_tenant\",\n",
        "        source_type=\"invoice\",\n",
        "        total=1_000_000.0,  # Outlier\n",
        "        parsed_by=\"csv\",\n",
        "        confidence=0.8,\n",
        "    )\n",
        "    service.feature_store.extract_features(doc)\n",
        "    assert service.detect_anomaly(doc)  # Should flag as anomaly\n",
        "\n",
        "# Smoke test\n",
        "def run_smoke_test():\n",
        "    \"\"\"Run AI services smoke tests.\"\"\"\n",
        "    import tempfile\n",
        "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
        "        global MODELS_DIR, CORRECTIONS_DIR\n",
        "        MODELS_DIR = Path(tmpdirname) / \"models\"\n",
        "        CORRECTIONS_DIR = Path(tmpdirname) / \"ml_corrections\"\n",
        "        test_vendor_normalization()\n",
        "        test_category_prediction()\n",
        "        test_anomaly_detection()\n",
        "    print(\"AI services smoke tests passed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_smoke_test()"
      ],
      "metadata": {
        "id": "etv_60nrVGD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cell_05_accounting_core.py\n",
        "\"\"\"\n",
        "LedgerOne Accounting Core\n",
        "========================\n",
        "Implements core accounting logic for AP, AR, and GL with Kenyan-specific rules (VAT, KRA compliance).\n",
        "Processes canonical documents into ledger entries, handles reconciliation, and generates trial balances.\n",
        "Key features:\n",
        "- AP: Invoice processing, payment scheduling, VAT tracking.\n",
        "- AR: Customer invoices, payment tracking.\n",
        "- GL: Double-entry bookkeeping, trial balance generation.\n",
        "- Kenyan-specific: VAT 16%, KRA ETR-compliant fields.\n",
        "- Audit logging to data/audit/{tenant}/.\n",
        "- Unit tests for accounting operations.\n",
        "\n",
        "Configuration:\n",
        "- Uses constants from cell_01_core_bootstrap.py.\n",
        "- Ledger data stored in data/ledger/{tenant}/.\n",
        "\n",
        "Extension points:\n",
        "- Add new account types in ChartOfAccounts.\n",
        "- Extend reconciliation rules in LedgerService.\n",
        "\"\"\"\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Optional\n",
        "from datetime import datetime\n",
        "import logging\n",
        "from pydantic import BaseModel, Field\n",
        "import pandas as pd\n",
        "from ledgerone_prototype.cell_01_core_bootstrap import setup_logging, LedgerOneError, TenantContext, config, KENYA_CONSTANTS\n",
        "from ledgerone_prototype.cell_03_canonical_schema_featurestore import CanonicalDocument\n",
        "\n",
        "# File paths\n",
        "LEDGER_DIR = Path(config.DATA_DIR) / \"ledger\"\n",
        "LEDGER_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Accounting models\n",
        "class Account(BaseModel):\n",
        "    account_id: str\n",
        "    name: str\n",
        "    type: str  # asset, liability, equity, revenue, expense\n",
        "    balance: float = 0.0\n",
        "\n",
        "class LedgerEntry(BaseModel):\n",
        "    entry_id: str\n",
        "    doc_id: str\n",
        "    tenant_id: str\n",
        "    account_id: str\n",
        "    debit: float = 0.0\n",
        "    credit: float = 0.0\n",
        "    date: str\n",
        "    description: str\n",
        "    vat_applicable: bool = False\n",
        "\n",
        "class ChartOfAccounts:\n",
        "    \"\"\"Kenyan-specific chart of accounts (aligned with KNBS/KRA).\"\"\"\n",
        "    ACCOUNTS = [\n",
        "        Account(account_id=\"1001\", name=\"Cash\", type=\"asset\"),\n",
        "        Account(account_id=\"2001\", name=\"Accounts Payable\", type=\"liability\"),\n",
        "        Account(account_id=\"2002\", name=\"VAT Payable\", type=\"liability\"),\n",
        "        Account(account_id=\"3001\", name=\"Accounts Receivable\", type=\"asset\"),\n",
        "        Account(account_id=\"4001\", name=\"Revenue\", type=\"revenue\"),\n",
        "        Account(account_id=\"5001\", name=\"Expenses\", type=\"expense\"),\n",
        "    ]\n",
        "\n",
        "class LedgerService:\n",
        "    \"\"\"Manages AP, AR, and GL operations.\"\"\"\n",
        "    def __init__(self, tenant_id: str):\n",
        "        self.tenant_id = tenant_id\n",
        "        self.logger = setup_logging(tenant_id)\n",
        "        self.accounts = {acc.account_id: acc for acc in ChartOfAccounts.ACCOUNTS}\n",
        "        self.ledger_path = LEDGER_DIR / tenant_id / \"ledger.jsonl\"\n",
        "\n",
        "    def process_ap(self, doc: CanonicalDocument) -> List[LedgerEntry]:\n",
        "        \"\"\"Process AP invoice into ledger entries.\"\"\"\n",
        "        try:\n",
        "            entries = []\n",
        "            entry_id = f\"entry_{hashlib.md5((doc.doc_id + str(datetime.utcnow())).encode()).hexdigest()}\"\n",
        "            # Debit expense, credit AP and VAT\n",
        "            entries.append(LedgerEntry(\n",
        "                entry_id=entry_id + \"_1\",\n",
        "                doc_id=doc.doc_id,\n",
        "                tenant_id=self.tenant_id,\n",
        "                account_id=\"5001\",  # Expenses\n",
        "                debit=doc.total,\n",
        "                credit=0.0,\n",
        "                date=doc.dates.get(\"issue\", datetime.utcnow().isoformat()),\n",
        "                description=f\"Invoice {doc.vendor.raw if doc.vendor else 'Unknown'}\",\n",
        "                vat_applicable=True,\n",
        "            ))\n",
        "            vat_amount = sum(vl.amount for vl in doc.vat_lines)\n",
        "            entries.append(LedgerEntry(\n",
        "                entry_id=entry_id + \"_2\",\n",
        "                doc_id=doc.doc_id,\n",
        "                tenant_id=self.tenant_id,\n",
        "                account_id=\"2002\",  # VAT Payable\n",
        "                debit=0.0,\n",
        "                credit=vat_amount,\n",
        "                date=doc.dates.get(\"issue\", datetime.utcnow().isoformat()),\n",
        "                description=\"VAT Payable\",\n",
        "                vat_applicable=True,\n",
        "            ))\n",
        "            entries.append(LedgerEntry(\n",
        "                entry_id=entry_id + \"_3\",\n",
        "                doc_id=doc.doc_id,\n",
        "                tenant_id=self.tenant_id,\n",
        "                account_id=\"2001\",  # Accounts Payable\n",
        "                debit=0.0,\n",
        "                credit=doc.total - vat_amount,\n",
        "                date=doc.dates.get(\"issue\", datetime.utcnow().isoformat()),\n",
        "                description=\"Accounts Payable\",\n",
        "            ))\n",
        "            self._save_entries(entries)\n",
        "            self.logger.info(f\"Processed AP for {doc.doc_id}\", extra={\"action\": \"process_ap\"})\n",
        "            return entries\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"AP processing failed: {str(e)}\", extra={\"action\": \"process_ap\"})\n",
        "            raise LedgerOneError(f\"AP processing failed: {str(e)}\")\n",
        "\n",
        "    def process_ar(self, doc: CanonicalDocument) -> List[LedgerEntry]:\n",
        "        \"\"\"Process AR invoice into ledger entries.\"\"\"\n",
        "        try:\n",
        "            entries = []\n",
        "            entry_id = f\"entry_{hashlib.md5((doc.doc_id + str(datetime.utcnow())).encode()).hexdigest()}\"\n",
        "            entries.append(LedgerEntry(\n",
        "                entry_id=entry_id + \"_1\",\n",
        "                doc_id=doc.doc_id,\n",
        "                tenant_id=self.tenant_id,\n",
        "                account_id=\"3001\",  # Accounts Receivable\n",
        "                debit=doc.total,\n",
        "                credit=0.0,\n",
        "                date=doc.dates.get(\"issue\", datetime.utcnow().isoformat()),\n",
        "                description=f\"AR Invoice {doc.vendor.raw if doc.vendor else 'Unknown'}\",\n",
        "            ))\n",
        "            entries.append(LedgerEntry(\n",
        "                entry_id=entry_id + \"_2\",\n",
        "                doc_id=doc.doc_id,\n",
        "                tenant_id=self.tenant_id,\n",
        "                account_id=\"4001\",  # Revenue\n",
        "                debit=0.0,\n",
        "                credit=doc.total,\n",
        "                date=doc.dates.get(\"issue\", datetime.utcnow().isoformat()),\n",
        "                description=\"Revenue\",\n",
        "            ))\n",
        "            self._save_entries(entries)\n",
        "            self.logger.info(f\"Processed AR for {doc.doc_id}\", extra={\"action\": \"process_ar\"})\n",
        "            return entries\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"AR processing failed: {str(e)}\", extra={\"action\": \"process_ar\"})\n",
        "            raise LedgerOneError(f\"AR processing failed: {str(e)}\")\n",
        "\n",
        "    def generate_trial_balance(self) -> Dict[str, float]:\n",
        "        \"\"\"Generate trial balance from ledger.\"\"\"\n",
        "        try:\n",
        "            ledger = self._load_ledger()\n",
        "            balances = {acc.account_id: 0.0 for acc in self.accounts.values()}\n",
        "            for entry in ledger:\n",
        "                balances[entry.account_id] += entry.debit - entry.credit\n",
        "            self.logger.info(\"Generated trial balance\", extra={\"action\": \"generate_trial_balance\"})\n",
        "            return balances\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Trial balance failed: {str(e)}\", extra={\"action\": \"generate_trial_balance\"})\n",
        "            raise LedgerOneError(f\"Trial balance failed: {str(e)}\")\n",
        "\n",
        "    def _save_entries(self, entries: List[LedgerEntry]):\n",
        "        \"\"\"Save ledger entries to JSONL.\"\"\"\n",
        "        self.ledger_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        with open(self.ledger_path, \"a\") as f:\n",
        "            for entry in entries:\n",
        "                f.write(json.dumps(entry.dict()) + \"\\n\")\n",
        "\n",
        "    def _load_ledger(self) -> List[LedgerEntry]:\n",
        "        \"\"\"Load ledger entries from JSONL.\"\"\"\n",
        "        if not self.ledger_path.exists():\n",
        "            return []\n",
        "        entries = []\n",
        "        with open(self.ledger_path) as f:\n",
        "            for line in f:\n",
        "                entries.append(LedgerEntry(**json.loads(line)))\n",
        "        return entries\n",
        "\n",
        "# FastAPI endpoints\n",
        "from fastapi import HTTPException\n",
        "@app.post(\"/accounting/ap\")\n",
        "async def process_ap_endpoint(doc: CanonicalDocument, tenant_id: str = Depends(get_current_user)):\n",
        "    \"\"\"Process AP invoice.\"\"\"\n",
        "    try:\n",
        "        service = LedgerService(tenant_id=tenant_id[\"tenant_id\"])\n",
        "        entries = service.process_ap(doc)\n",
        "        return [e.dict() for e in entries]\n",
        "    except LedgerOneError as e:\n",
        "        raise HTTPException(status_code=400, detail=str(e))\n",
        "\n",
        "@app.post(\"/accounting/ar\")\n",
        "async def process_ar_endpoint(doc: CanonicalDocument, tenant_id: str = Depends(get_current_user)):\n",
        "    \"\"\"Process AR invoice.\"\"\"\n",
        "    try:\n",
        "        service = LedgerService(tenant_id=tenant_id[\"tenant_id\"])\n",
        "        entries = service.process_ar(doc)\n",
        "        return [e.dict() for e in entries]\n",
        "    except LedgerOneError as e:\n",
        "        raise HTTPException(status_code=400, detail=str(e))\n",
        "\n",
        "@app.get(\"/accounting/trial_balance\")\n",
        "async def trial_balance_endpoint(tenant_id: str = Depends(get_current_user)):\n",
        "    \"\"\"Generate trial balance.\"\"\"\n",
        "    try:\n",
        "        service = LedgerService(tenant_id=tenant_id[\"tenant_id\"])\n",
        "        return service.generate_trial_balance()\n",
        "    except LedgerOneError as e:\n",
        "        raise HTTPException(status_code=400, detail=str(e))\n",
        "\n",
        "# Unit tests\n",
        "def test_ap_processing(tmp_path):\n",
        "    \"\"\"Test AP processing.\"\"\"\n",
        "    LEDGER_DIR = tmp_path / \"ledger\"\n",
        "    LEDGER_DIR.mkdir(parents=True)\n",
        "    doc = CanonicalDocument(\n",
        "        doc_id=\"test_001\",\n",
        "        tenant_id=\"test_tenant\",\n",
        "        source_type=\"invoice\",\n",
        "        vendor=Vendor(raw=\"Safaricom\"),\n",
        "        total=1000.0,\n",
        "        vat_lines=[VatLine(amount=160.0)],\n",
        "        parsed_by=\"csv\",\n",
        "        confidence=0.8,\n",
        "        dates={\"issue\": \"2025-09-10\"},\n",
        "    )\n",
        "    service = LedgerService(tenant_id=\"test_tenant\")\n",
        "    entries = service.process_ap(doc)\n",
        "    assert len(entries) == 3\n",
        "    assert entries[0].debit == 1000.0  # Expenses\n",
        "    assert entries[1].credit == 160.0   # VAT Payable\n",
        "    assert entries[2].credit == 840.0   # AP\n",
        "\n",
        "def test_trial_balance(tmp_path):\n",
        "    \"\"\"Test trial balance generation.\"\"\"\n",
        "    LEDGER_DIR = tmp_path / \"ledger\"\n",
        "    LEDGER_DIR.mkdir(parents=True)\n",
        "    service = LedgerService(tenant_id=\"test_tenant\")\n",
        "    doc = CanonicalDocument(\n",
        "        doc_id=\"test_001\",\n",
        "        tenant_id=\"test_tenant\",\n",
        "        source_type=\"invoice\",\n",
        "        total=1000.0,\n",
        "        parsed_by=\"csv\",\n",
        "        confidence=0.8,\n",
        "    )\n",
        "    service.process_ap(doc)\n",
        "    balances = service.generate_trial_balance()\n",
        "    assert balances[\"5001\"] == 1000.0  # Expenses\n",
        "    assert balances[\"2001\"] == -1000.0  # AP\n",
        "\n",
        "# Smoke test\n",
        "def run_smoke_test():\n",
        "    \"\"\"Run accounting core smoke tests.\"\"\"\n",
        "    import tempfile\n",
        "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
        "        global LEDGER_DIR\n",
        "        LEDGER_DIR = Path(tmpdirname) / \"ledger\"\n",
        "        test_ap_processing(Path(tmpdirname))\n",
        "        test_trial_balance(Path(tmpdirname))\n",
        "    print(\"Accounting core smoke tests passed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_smoke_test()"
      ],
      "metadata": {
        "id": "YLLEH40GVS5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cell_06_payroll_engine.py\n",
        "\"\"\"\n",
        "LedgerOne Payroll Engine\n",
        "=======================\n",
        "Implements payroll processing with Kenyan-specific statutory deductions (PAYE, NSSF, SHA,\n",
        "Housing Levy) based on 2025 KRA/SHA/NSSF rules. Generates payslips and logs audit trails.\n",
        "Key features:\n",
        "- Calculates PAYE, NSSF (Tier I/II), SHA (2.75%, min 300 KES), Housing Levy (1.5%).\n",
        "- Generates KRA-compliant payslips.\n",
        "- Integrates with CanonicalDocument from cell_03.\n",
        "- Audit logging to data/audit/{tenant}/.\n",
        "- Unit tests for calculations and payslips.\n",
        "\n",
        "Configuration:\n",
        "- Uses KENYA_CONSTANTS from cell_01_core_bootstrap.py.\n",
        "- Payslips stored in data/payslips/{tenant}/.\n",
        "\n",
        "Extension points:\n",
        "- Add new deduction types in PayrollCalculator.\n",
        "- Extend payslip templates in PayslipGenerator.\n",
        "\"\"\"\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Optional\n",
        "from datetime import datetime\n",
        "import logging\n",
        "from pydantic import BaseModel\n",
        "import pandas as pd\n",
        "from ledgerone_prototype.cell_01_core_bootstrap import setup_logging, LedgerOneError, config, KENYA_CONSTANTS\n",
        "from ledgerone_prototype.cell_03_canonical_schema_featurestore import CanonicalDocument\n",
        "\n",
        "# File paths\n",
        "PAYSLIPS_DIR = Path(config.DATA_DIR) / \"payslips\"\n",
        "PAYSLIPS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Payroll models\n",
        "class Payslip(BaseModel):\n",
        "    employee_id: str\n",
        "    name: str\n",
        "    period: str\n",
        "    gross_pay: float\n",
        "    paye: float\n",
        "    nssf: float\n",
        "    sha: float\n",
        "    housing_levy: float\n",
        "    net_pay: float\n",
        "    other_deductions: float = 0.0\n",
        "    tenant_id: str\n",
        "\n",
        "class PayrollCalculator:\n",
        "    \"\"\"Calculates Kenyan statutory deductions.\"\"\"\n",
        "    def __init__(self, tenant_id: str):\n",
        "        self.tenant_id = tenant_id\n",
        "        self.logger = setup_logging(tenant_id)\n",
        "\n",
        "    def calculate_paye(self, taxable_income: float) -> float:\n",
        "        \"\"\"Calculate PAYE based on 2025 KRA bands.\"\"\"\n",
        "        # Source: https://www.kra.go.ke/individual/calculating-tax/paye/understanding-paye\n",
        "        paye = 0.0\n",
        "        for band in KENYA_CONSTANTS[\"PAYE_BANDS\"]:\n",
        "            if taxable_income > band[\"min\"]:\n",
        "                taxable_in_band = min(taxable_income, band[\"max\"]) - band[\"min\"]\n",
        "                paye += taxable_in_band * band[\"rate\"]\n",
        "        self.logger.info(f\"PAYE calculated: {paye}\", extra={\"action\": \"calculate_paye\"})\n",
        "        return round(paye, 2)\n",
        "\n",
        "    def calculate_nssf(self, pensionable_pay: float) -> float:\n",
        "        \"\"\"Calculate NSSF (Tier I and II).\"\"\"\n",
        "        # Source: https://www.nssf.or.ke/ (2025 rates)\n",
        "        tier_i = min(pensionable_pay, KENYA_CONSTANTS[\"NSSF_TIER_I_MAX\"]) * 0.06\n",
        "        tier_ii = min(max(pensionable_pay - KENYA_CONSTANTS[\"NSSF_TIER_I_MAX\"], 0),\n",
        "                      KENYA_CONSTANTS[\"NSSF_TIER_II_MAX\"]) * 0.06\n",
        "        nssf = tier_i + tier_ii\n",
        "        self.logger.info(f\"NSSF calculated: {nssf}\", extra={\"action\": \"calculate_nssf\"})\n",
        "        return round(nssf, 2)\n",
        "\n",
        "    def calculate_sha(self, gross_pay: float) -> float:\n",
        "        \"\"\"Calculate SHA contribution (replaced NHIF in 2024).\"\"\"\n",
        "        # Source: https://www.sha.go.ke/ (2025 rates)\n",
        "        sha = max(gross_pay * KENYA_CONSTANTS[\"SHA_RATE\"], KENYA_CONSTANTS[\"SHA_MIN\"])\n",
        "        self.logger.info(f\"SHA calculated: {sha}\", extra={\"action\": \"calculate_sha\"})\n",
        "        return round(sha, 2)\n",
        "\n",
        "    def calculate_housing_levy(self, gross_pay: float) -> float:\n",
        "        \"\"\"Calculate Housing Levy.\"\"\"\n",
        "        # Source: https://www.kra.go.ke/ (2025 rates)\n",
        "        levy = gross_pay * KENYA_CONSTANTS[\"HOUSING_LEVY\"]\n",
        "        self.logger.info(f\"Housing Levy calculated: {levy}\", extra={\"action\": \"calculate_housing_levy\"})\n",
        "        return round(levy, 2)\n",
        "\n",
        "    def process_payroll(self, doc: CanonicalDocument) -> Payslip:\n",
        "        \"\"\"Process payroll document into payslip.\"\"\"\n",
        "        try:\n",
        "            record = doc.dict()\n",
        "            gross_pay = record[\"total\"]\n",
        "            paye = self.calculate_paye(gross_pay)\n",
        "            nssf = self.calculate_nssf(gross_pay)\n",
        "            sha = self.calculate_sha(gross_pay)\n",
        "            housing_levy = self.calculate_housing_levy(gross_pay)\n",
        "            other_deductions = record.get(\"deductions\", 0.0)\n",
        "            net_pay = gross_pay - paye - nssf - sha - housing_levy - other_deductions\n",
        "\n",
        "            payslip = Payslip(\n",
        "                employee_id=record[\"employee_id\"],\n",
        "                name=record[\"vendor\"][\"raw\"],\n",
        "                period=datetime.utcnow().strftime(\"%Y-%m\"),\n",
        "                gross_pay=gross_pay,\n",
        "                paye=paye,\n",
        "                nssf=nssf,\n",
        "                sha=sha,\n",
        "                housing_levy=housing_levy,\n",
        "                net_pay=net_pay,\n",
        "                other_deductions=other_deductions,\n",
        "                tenant_id=self.tenant_id,\n",
        "            )\n",
        "            self._save_payslip(payslip)\n",
        "            self.logger.info(f\"Processed payslip for {record['employee_id']}\", extra={\"action\": \"process_payroll\"})\n",
        "            return payslip\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Payroll processing failed: {str(e)}\", extra={\"action\": \"process_payroll\"})\n",
        "            raise LedgerOneError(f\"Payroll processing failed: {str(e)}\")\n",
        "\n",
        "    def _save_payslip(self, payslip: Payslip):\n",
        "        \"\"\"Save payslip to JSON.\"\"\"\n",
        "        path = PAYSLIPS_DIR / self.tenant_id / f\"{payslip.employee_id}_{payslip.period}.json\"\n",
        "        path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        with open(path, \"w\") as f:\n",
        "            json.dump(payslip.dict(), f, indent=2)\n",
        "\n",
        "# FastAPI endpoints\n",
        "from fastapi import HTTPException\n",
        "@app.post(\"/payroll/process\")\n",
        "async def process_payroll_endpoint(doc: CanonicalDocument, tenant_id: str = Depends(get_current_user)):\n",
        "    \"\"\"Process payroll document.\"\"\"\n",
        "    try:\n",
        "        calculator = PayrollCalculator(tenant_id=tenant_id[\"tenant_id\"])\n",
        "        payslip = calculator.process_payroll(doc)\n",
        "        return payslip.dict()\n",
        "    except LedgerOneError as e:\n",
        "        raise HTTPException(status_code=400, detail=str(e))\n",
        "\n",
        "@app.get(\"/payroll/payslip/{employee_id}/{period}\")\n",
        "async def get_payslip(employee_id: str, period: str, tenant_id: str = Depends(get_current_user)):\n",
        "    \"\"\"Retrieve payslip.\"\"\"\n",
        "    try:\n",
        "        path = PAYSLIPS_DIR / tenant_id[\"tenant_id\"] / f\"{employee_id}_{period}.json\"\n",
        "        if not path.exists():\n",
        "            raise LedgerOneError(\"Payslip not found\")\n",
        "        with open(path) as f:\n",
        "            return json.load(f)\n",
        "    except LedgerOneError as e:\n",
        "        raise HTTPException(status_code=404, detail=str(e))\n",
        "\n",
        "# Unit tests\n",
        "def test_payroll_calculations():\n",
        "    \"\"\"Test payroll calculations.\"\"\"\n",
        "    calculator = PayrollCalculator(tenant_id=\"test_tenant\")\n",
        "    gross_pay = 50000.0\n",
        "    paye = calculator.calculate_paye(gross_pay)\n",
        "    nssf = calculator.calculate_nssf(gross_pay)\n",
        "    sha = calculator.calculate_sha(gross_pay)\n",
        "    housing_levy = calculator.calculate_housing_levy(gross_pay)\n",
        "    assert paye == 9125.0  # Based on 2025 PAYE bands\n",
        "    assert nssf == 2580.0  # Tier I (7k@6%) + Tier II (36k@6%)\n",
        "    assert sha == 1375.0   # 2.75% of 50k\n",
        "    assert housing_levy == 750.0  # 1.5% of 50k\n",
        "\n",
        "def test_payslip_generation(tmp_path):\n",
        "    \"\"\"Test payslip generation.\"\"\"\n",
        "    PAYSLIPS_DIR = tmp_path / \"payslips\"\n",
        "    PAYSLIPS_DIR.mkdir(parents=True)\n",
        "    doc = CanonicalDocument(\n",
        "        doc_id=\"test_001\",\n",
        "        tenant_id=\"test_tenant\",\n",
        "        source_type=\"payroll\",\n",
        "        vendor=Vendor(raw=\"John Doe\"),\n",
        "        total=50000.0,\n",
        "        parsed_by=\"xlsx\",\n",
        "        confidence=0.85,\n",
        "        employee_id=\"E001\",\n",
        "        deductions=2000.0,\n",
        "    )\n",
        "    calculator = PayrollCalculator(tenant_id=\"test_tenant\")\n",
        "    payslip = calculator.process_payroll(doc)\n",
        "    assert payslip.net_pay == 36170.0  # 50k - PAYE - NSSF - SHA - Housing - Deductions\n",
        "    assert (PAYSLIPS_DIR / \"test_tenant\" / f\"E001_{datetime.utcnow().strftime('%Y-%m')}.json\").exists()\n",
        "\n",
        "# Smoke test\n",
        "def run_smoke_test():\n",
        "    \"\"\"Run payroll engine smoke tests.\"\"\"\n",
        "    import tempfile\n",
        "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
        "        global PAYSLIPS_DIR\n",
        "        PAYSLIPS_DIR = Path(tmpdirname) / \"payslips\"\n",
        "        test_payroll_calculations()\n",
        "        test_payslip_generation(Path(tmpdirname))\n",
        "    print(\"Payroll engine smoke tests passed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_smoke_test()"
      ],
      "metadata": {
        "id": "9qCRr8M2VYpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cell_07_workflow_rbac.py\n",
        "\"\"\"\n",
        "LedgerOne Workflow and RBAC\n",
        "==========================\n",
        "Implements Role-Based Access Control (RBAC) and approval workflows for document processing.\n",
        "Supports Kenyan-specific role templates and dynamic approver resolution. Manages task lifecycle\n",
        "with audit logging and deadline/escalation simulation.\n",
        "Key features:\n",
        "- RBAC: Role templates (CEO, CFO, AP Clerk, etc.), permission checks.\n",
        "- Workflow: Task creation, assignment, approval/rejection with state transitions.\n",
        "- Scheduler: Thread-based deadline/escalation simulation.\n",
        "- Audit logging to data/audit/{tenant}/.\n",
        "- Unit tests for RBAC and workflows.\n",
        "\n",
        "Configuration:\n",
        "- Uses RoleEnum from cell_01_core_bootstrap.py.\n",
        "- Tasks stored in data/tasks/{tenant}/.\n",
        "\n",
        "Extension points:\n",
        "- Add new roles in RoleEnum.\n",
        "- Extend workflow states in Task model.\n",
        "- Replace scheduler with Celery in production.\n",
        "\"\"\"\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Optional\n",
        "from datetime import datetime, timedelta\n",
        "import logging\n",
        "import threading\n",
        "from pydantic import BaseModel\n",
        "from ledgerone_prototype.cell_01_core_bootstrap import setup_logging, LedgerOneError, config, RoleEnum, get_current_user\n",
        "from ledgerone_prototype.cell_03_canonical_schema_featurestore import CanonicalDocument\n",
        "\n",
        "# File paths\n",
        "TASKS_DIR = Path(config.DATA_DIR) / \"tasks\"\n",
        "TASKS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Workflow models\n",
        "class Task(BaseModel):\n",
        "    task_id: str\n",
        "    doc_id: str\n",
        "    tenant_id: str\n",
        "    assigned_to: Optional[str] = None\n",
        "    status: str = \"pending\"  # pending, claimed, completed, rejected\n",
        "    created_at: str\n",
        "    deadline: str\n",
        "    approver_role: RoleEnum\n",
        "    action: str  # approve, reject\n",
        "    audit_trail: List[Dict[str, str]] = []\n",
        "\n",
        "class RBACRule(BaseModel):\n",
        "    role: RoleEnum\n",
        "    permissions: List[str]  # e.g., [\"create_task\", \"approve_invoice\"]\n",
        "\n",
        "class WorkflowService:\n",
        "    \"\"\"Manages approval workflows and RBAC.\"\"\"\n",
        "    def __init__(self, tenant_id: str):\n",
        "        self.tenant_id = tenant_id\n",
        "        self.logger = setup_logging(tenant_id)\n",
        "        self.rbac_rules = self._load_rbac_rules()\n",
        "        self.scheduler_thread = threading.Thread(target=self._run_scheduler)\n",
        "        self.scheduler_thread.daemon = True\n",
        "        self.scheduler_thread.start()\n",
        "\n",
        "    def _load_rbac_rules(self) -> List[RBACRule]:\n",
        "        \"\"\"Load Kenyan-specific RBAC rules (aligned with KNBS org structures).\"\"\"\n",
        "        # Source: https://www.knbs.or.ke/ (business role templates)\n",
        "        return [\n",
        "            RBACRule(role=RoleEnum.SUPER_ADMIN, permissions=[\"all\"]),\n",
        "            RBACRule(role=RoleEnum.COMPANY_ADMIN, permissions=[\"manage_users\", \"configure_workflow\"]),\n",
        "            RBACRule(role=RoleEnum.CEO, permissions=[\"approve_invoice\", \"view_reports\"]),\n",
        "            RBACRule(role=RoleEnum.CFO, permissions=[\"approve_invoice\", \"view_reports\"]),\n",
        "            RBACRule(role=RoleEnum.FINANCE_MGR, permissions=[\"create_task\", \"approve_invoice\"]),\n",
        "            RBACRule(role=RoleEnum.AP_CLERK, permissions=[\"create_task\", \"view_tasks\"]),\n",
        "            RBACRule(role=RoleEnum.HR_MGR, permissions=[\"process_payroll\"]),\n",
        "            RBACRule(role=RoleEnum.BRANCH_MGR, permissions=[\"view_reports\"]),\n",
        "        ]\n",
        "\n",
        "    def create_task(self, doc: CanonicalDocument, approver_role: RoleEnum) -> Task:\n",
        "        \"\"\"Create approval task for document.\"\"\"\n",
        "        try:\n",
        "            task_id = f\"task_{hashlib.md5((doc.doc_id + str(datetime.utcnow())).encode()).hexdigest()}\"\n",
        "            task = Task(\n",
        "                task_id=task_id,\n",
        "                doc_id=doc.doc_id,\n",
        "                tenant_id=self.tenant_id,\n",
        "                approver_role=approver_role,\n",
        "                created_at=datetime.utcnow().isoformat(),\n",
        "                deadline=(datetime.utcnow() + timedelta(days=7)).isoformat(),\n",
        "                audit_trail=[{\"action\": \"created\", \"timestamp\": datetime.utcnow().isoformat()}],\n",
        "            )\n",
        "            self._save_task(task)\n",
        "            self.logger.info(f\"Created task {task_id}\", extra={\"action\": \"create_task\"})\n",
        "            return task\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Task creation failed: {str(e)}\", extra={\"action\": \"create_task\"})\n",
        "            raise LedgerOneError(f\"Task creation failed: {str(e)}\")\n",
        "\n",
        "    def assign_task(self, task_id: str, user_id: str, role: RoleEnum):\n",
        "        \"\"\"Assign task to user.\"\"\"\n",
        "        try:\n",
        "            task = self._load_task(task_id)\n",
        "            if not self._check_permission(role, \"create_task\"):\n",
        "                raise LedgerOneError(\"Permission denied\")\n",
        "            task.assigned_to = user_id\n",
        "            task.status = \"claimed\"\n",
        "            task.audit_trail.append({\"action\": \"assigned\", \"user_id\": user_id, \"timestamp\": datetime.utcnow().isoformat()})\n",
        "            self._save_task(task)\n",
        "            self.logger.info(f\"Assigned task {task_id}\", extra={\"action\": \"assign_task\"})\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Task assignment failed: {str(e)}\", extra={\"action\": \"assign_task\"})\n",
        "            raise LedgerOneError(f\"Task assignment failed: {str(e)}\")\n",
        "\n",
        "    def process_task(self, task_id: str, user_id: str, role: RoleEnum, action: str):\n",
        "        \"\"\"Process task (approve/reject).\"\"\"\n",
        "        try:\n",
        "            task = self._load_task(task_id)\n",
        "            if not self._check_permission(role, \"approve_invoice\"):\n",
        "                raise LedgerOneError(\"Permission denied\")\n",
        "            if task.assigned_to != user_id:\n",
        "                raise LedgerOneError(\"Task not assigned to user\")\n",
        "            task.status = \"completed\" if action == \"approve\" else \"rejected\"\n",
        "            task.audit_trail.append({\"action\": action, \"user_id\": user_id, \"timestamp\": datetime.utcnow().isoformat()})\n",
        "            self._save_task(task)\n",
        "            self.logger.info(f\"Processed task {task_id}: {action}\", extra={\"action\": \"process_task\"})\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Task processing failed: {str(e)}\", extra={\"action\": \"process_task\"})\n",
        "            raise LedgerOneError(f\"Task processing failed: {str(e)}\")\n",
        "\n",
        "    def _check_permission(self, role: RoleEnum, permission: str) -> bool:\n",
        "        \"\"\"Check if role has permission.\"\"\"\n",
        "        for rule in self.rbac_rules:\n",
        "            if rule.role == role and (permission in rule.permissions or \"all\" in rule.permissions):\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def _save_task(self, task: Task):\n",
        "        \"\"\"Save task to JSON.\"\"\"\n",
        "        path = TASKS_DIR / self.tenant_id / f\"{task.task_id}.json\"\n",
        "        path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        with open(path, \"w\") as f:\n",
        "            json.dump(task.dict(), f, indent=2)\n",
        "\n",
        "    def _load_task(self, task_id: str) -> Task:\n",
        "        \"\"\"Load task from JSON.\"\"\"\n",
        "        path = TASKS_DIR / self.tenant_id / f\"{task_id}.json\"\n",
        "        if not path.exists():\n",
        "            raise LedgerOneError(\"Task not found\")\n",
        "        with open(path) as f:\n",
        "            return Task(**json.load(f))\n",
        "\n",
        "    def _run_scheduler(self):\n",
        "        \"\"\"Simulate deadline/escalation (thread-based).\"\"\"\n",
        "        while True:\n",
        "            for task_file in (TASKS_DIR / self.tenant_id).glob(\"*.json\"):\n",
        "                task = self._load_task(task_file.stem)\n",
        "                if task.status in [\"pending\", \"claimed\"] and datetime.fromisoformat(task.deadline) < datetime.utcnow():\n",
        "                    task.audit_trail.append({\"action\": \"escalated\", \"timestamp\": datetime.utcnow().isoformat()})\n",
        "                    task.status = \"escalated\"\n",
        "                    self._save_task(task)\n",
        "                    self.logger.info(f\"Escalated task {task.task_id}\", extra={\"action\": \"scheduler\"})\n",
        "            time.sleep(60)  # Check every minute\n",
        "\n",
        "# FastAPI endpoints\n",
        "from fastapi import HTTPException\n",
        "@app.post(\"/workflow/task\")\n",
        "async def create_task_endpoint(doc: CanonicalDocument, approver_role: RoleEnum, current_user: dict = Depends(get_current_user)):\n",
        "    \"\"\"Create approval task.\"\"\"\n",
        "    try:\n",
        "        service = WorkflowService(tenant_id=current_user[\"tenant_id\"])\n",
        "        if not service._check_permission(RoleEnum(current_user[\"role\"]), \"create_task\"):\n",
        "            raise LedgerOneError(\"Permission denied\")\n",
        "        task = service.create_task(doc, approver_role)\n",
        "        return task.dict()\n",
        "    except LedgerOneError as e:\n",
        "        raise HTTPException(status_code=403, detail=str(e))\n",
        "\n",
        "@app.post(\"/workflow/task/{task_id}/assign\")\n",
        "async def assign_task_endpoint(task_id: str, user_id: str, current_user: dict = Depends(get_current_user)):\n",
        "    \"\"\"Assign task to user.\"\"\"\n",
        "    try:\n",
        "        service = WorkflowService(tenant_id=current_user[\"tenant_id\"])\n",
        "        service.assign_task(task_id, user_id, RoleEnum(current_user[\"role\"]))\n",
        "        return {\"status\": \"task assigned\"}\n",
        "    except LedgerOneError as e:\n",
        "        raise HTTPException(status_code=403, detail=str(e))\n",
        "\n",
        "@app.post(\"/workflow/task/{task_id}/process\")\n",
        "async def process_task_endpoint(task_id: str, action: str, current_user: dict = Depends(get_current_user)):\n",
        "    \"\"\"Process task (approve/reject).\"\"\"\n",
        "    try:\n",
        "        service = WorkflowService(tenant_id=current_user[\"tenant_id\"])\n",
        "        service.process_task(task_id, current_user[\"user_id\"], RoleEnum(current_user[\"role\"]), action)\n",
        "        return {\"status\": f\"task {action}\"}\n",
        "    except LedgerOneError as e:\n",
        "        raise HTTPException(status_code=403, detail=str(e))\n",
        "\n",
        "# Unit tests\n",
        "def test_rbac():\n",
        "    \"\"\"Test RBAC permissions.\"\"\"\n",
        "    service = WorkflowService(tenant_id=\"test_tenant\")\n",
        "    assert service._check_permission(RoleEnum.SUPER_ADMIN, \"all\")\n",
        "    assert service._check_permission(RoleEnum.AP_CLERK, \"create_task\")\n",
        "    assert not service._check_permission(RoleEnum.AP_CLERK, \"approve_invoice\")\n",
        "\n",
        "def test_workflow(tmp_path):\n",
        "    \"\"\"Test task creation and processing.\"\"\"\n",
        "    TASKS_DIR = tmp_path / \"tasks\"\n",
        "    TASKS_DIR.mkdir(parents=True)\n",
        "    doc = CanonicalDocument(\n",
        "        doc_id=\"test_001\",\n",
        "        tenant_id=\"test_tenant\",\n",
        "        source_type=\"invoice\",\n",
        "        total=1000.0,\n",
        "        parsed_by=\"csv\",\n",
        "        confidence=0.8,\n",
        "    )\n",
        "    service = WorkflowService(tenant_id=\"test_tenant\")\n",
        "    task = service.create_task(doc, RoleEnum.FINANCE_MGR)\n",
        "    assert task.status == \"pending\"\n",
        "    service.assign_task(task.task_id, \"user_001\", RoleEnum.FINANCE_MGR)\n",
        "    task = service._load_task(task.task_id)\n",
        "    assert task.status == \"claimed\"\n",
        "    service.process_task(task.task_id, \"user_001\", RoleEnum.FINANCE_MGR, \"approve\")\n",
        "    task = service._load_task(task.task_id)\n",
        "    assert task.status == \"completed\"\n",
        "\n",
        "# Smoke test\n",
        "def run_smoke_test():\n",
        "    \"\"\"Run workflow and RBAC smoke tests.\"\"\"\n",
        "    import tempfile\n",
        "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
        "        global TASKS_DIR\n",
        "        TASKS_DIR = Path(tmpdirname) / \"tasks\"\n",
        "        test_rbac()\n",
        "        test_workflow(Path(tmpdirname))\n",
        "    print(\"Workflow and RBAC smoke tests passed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_smoke_test()"
      ],
      "metadata": {
        "id": "0tWQXBGYVjk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cell_08_admin_console.py\n",
        "\"\"\"\n",
        "LedgerOne Admin Console\n",
        "======================\n",
        "Implements the admin console for tenant management, role assignments, connector toggles,\n",
        "and AI model retraining. Provides Streamlit UI for tenant admins and super admin with\n",
        "global access (impersonation, global retrain).\n",
        "Key features:\n",
        "- Tenant admin: Manage users, roles, approval thresholds, connectors.\n",
        "- Super admin: Global access, impersonation, trigger global retrain.\n",
        "- Streamlit UI with role-based dashboards.\n",
        "- Audit logging to data/audit/{tenant}/.\n",
        "- Unit tests for admin operations.\n",
        "\n",
        "Configuration:\n",
        "- Uses RoleEnum from cell_01_core_bootstrap.py.\n",
        "- Settings stored in data/settings/{tenant}/.\n",
        "\n",
        "Extension points:\n",
        "- Add new admin features in AdminConsole.\n",
        "- Extend UI components in Streamlit functions.\n",
        "\"\"\"\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Optional\n",
        "from datetime import datetime\n",
        "import logging\n",
        "import streamlit as st\n",
        "from pydantic import BaseModel\n",
        "from ledgerone_prototype.cell_01_core_bootstrap import setup_logging, LedgerOneError, config, RoleEnum, get_current_user, User\n",
        "from ledgerone_prototype.cell_04_ai_services import AIService\n",
        "\n",
        "# File paths\n",
        "SETTINGS_DIR = Path(config.DATA_DIR) / \"settings\"\n",
        "SETTINGS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Admin models\n",
        "class TenantSettings(BaseModel):\n",
        "    tenant_id: str\n",
        "    approval_threshold: float = 10000.0  # KES\n",
        "    active_connectors: List[str] = [\"mpesa\", \"bank_csv\", \"tesseract\"]\n",
        "\n",
        "class AdminConsole:\n",
        "    \"\"\"Manages tenant admin and super admin operations.\"\"\"\n",
        "    def __init__(self, tenant_id: str):\n",
        "        self.tenant_id = tenant_id\n",
        "        self.logger = setup_logging(tenant_id)\n",
        "        self.ai_service = AIService(tenant_id)\n",
        "\n",
        "    def assign_role(self, user_id: str, role: RoleEnum, current_user: dict):\n",
        "        \"\"\"Assign role to user.\"\"\"\n",
        "        try:\n",
        "            if not self._check_admin_privileges(current_user[\"role\"]):\n",
        "                raise LedgerOneError(\"Permission denied\")\n",
        "            user = self._load_user(user_id)\n",
        "            user.role = role\n",
        "            self._save_user(user)\n",
        "            self.logger.info(f\"Assigned role {role} to {user_id}\", extra={\"action\": \"assign_role\"})\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Role assignment failed: {str(e)}\", extra={\"action\": \"assign_role\"})\n",
        "            raise LedgerOneError(f\"Role assignment failed: {str(e)}\")\n",
        "\n",
        "    def set_approval_threshold(self, threshold: float, current_user: dict):\n",
        "        \"\"\"Set approval threshold for tenant.\"\"\"\n",
        "        try:\n",
        "            if not self._check_admin_privileges(current_user[\"role\"]):\n",
        "                raise LedgerOneError(\"Permission denied\")\n",
        "            settings = self._load_settings()\n",
        "            settings.approval_threshold = threshold\n",
        "            self._save_settings(settings)\n",
        "            self.logger.info(f\"Set approval threshold to {threshold}\", extra={\"action\": \"set_approval_threshold\"})\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Threshold setting failed: {str(e)}\", extra={\"action\": \"set_approval_threshold\"})\n",
        "            raise LedgerOneError(f\"Threshold setting failed: {str(e)}\")\n",
        "\n",
        "    def toggle_connector(self, connector: str, enable: bool, current_user: dict):\n",
        "        \"\"\"Toggle connector (e.g., mpesa, tesseract).\"\"\"\n",
        "        try:\n",
        "            if not self._check_admin_privileges(current_user[\"role\"]):\n",
        "                raise LedgerOneError(\"Permission denied\")\n",
        "            settings = self._load_settings()\n",
        "            if enable and connector not in settings.active_connectors:\n",
        "                settings.active_connectors.append(connector)\n",
        "            elif not enable and connector in settings.active_connectors:\n",
        "                settings.active_connectors.remove(connector)\n",
        "            self._save_settings(settings)\n",
        "            self.logger.info(f\"Toggled connector {connector}: {enable}\", extra={\"action\": \"toggle_connector\"})\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Connector toggle failed: {str(e)}\", extra={\"action\": \"toggle_connector\"})\n",
        "            raise LedgerOneError(f\"Connector toggle failed: {str(e)}\")\n",
        "\n",
        "    def trigger_retrain(self, current_user: dict):\n",
        "        \"\"\"Trigger AI model retraining.\"\"\"\n",
        "        try:\n",
        "            if not self._check_admin_privileges(current_user[\"role\"]):\n",
        "                raise LedgerOneError(\"Permission denied\")\n",
        "            # Stub: Retrain using corrections (expanded in cell_04)\n",
        "            self.ai_service._initialize_models()  # Reinitialize for simplicity\n",
        "            self.logger.info(\"Triggered model retrain\", extra={\"action\": \"trigger_retrain\"})\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Retrain failed: {str(e)}\", extra={\"action\": \"trigger_retrain\"})\n",
        "            raise LedgerOneError(f\"Retrain failed: {str(e)}\")\n",
        "\n",
        "    def impersonate_user(self, user_id: str, current_user: dict) -> dict:\n",
        "        \"\"\"Super admin impersonation.\"\"\"\n",
        "        try:\n",
        "            if current_user[\"role\"] != RoleEnum.SUPER_ADMIN:\n",
        "                raise LedgerOneError(\"Only super admin can impersonate\")\n",
        "            user = self._load_user(user_id)\n",
        "            self.logger.info(f\"Impersonated user {user_id}\", extra={\"action\": \"impersonate_user\"})\n",
        "            return user.dict()\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Impersonation failed: {str(e)}\", extra={\"action\": \"impersonate_user\"})\n",
        "            raise LedgerOneError(f\"Impersonation failed: {str(e)}\")\n",
        "\n",
        "    def _check_admin_privileges(self, role: str) -> bool:\n",
        "        \"\"\"Check if user has admin privileges.\"\"\"\n",
        "        return role in [RoleEnum.SUPER_ADMIN, RoleEnum.COMPANY_ADMIN]\n",
        "\n",
        "    def _load_user(self, user_id: str) -> User:\n",
        "        \"\"\"Load user (stub, expanded in cell_10).\"\"\"\n",
        "        path = SETTINGS_DIR / self.tenant_id / \"users.json\"\n",
        "        if not path.exists():\n",
        "            raise LedgerOneError(\"User not found\")\n",
        "        with open(path) as f:\n",
        "            users = json.load(f)\n",
        "            for user_data in users:\n",
        "                if user_data[\"user_id\"] == user_id:\n",
        "                    return User(**user_data)\n",
        "        raise LedgerOneError(\"User not found\")\n",
        "\n",
        "    def _save_user(self, user: User):\n",
        "        \"\"\"Save user (stub, expanded in cell_10).\"\"\"\n",
        "        path = SETTINGS_DIR / self.tenant_id / \"users.json\"\n",
        "        path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        users = [self._load_user(u[\"user_id\"]).dict() for u in json.load(open(path))] if path.exists() else []\n",
        "        users = [u for u in users if u[\"user_id\"] != user.user_id] + [user.dict()]\n",
        "        with open(path, \"w\") as f:\n",
        "            json.dump(users, f, indent=2)\n",
        "\n",
        "    def _load_settings(self) -> TenantSettings:\n",
        "        \"\"\"Load tenant settings.\"\"\"\n",
        "        path = SETTINGS_DIR / self.tenant_id / \"settings.json\"\n",
        "        if not path.exists():\n",
        "            return TenantSettings(tenant_id=self.tenant_id)\n",
        "        with open(path) as f:\n",
        "            return TenantSettings(**json.load(f))\n",
        "\n",
        "    def _save_settings(self, settings: TenantSettings):\n",
        "        \"\"\"Save tenant settings.\"\"\"\n",
        "        path = SETTINGS_DIR / self.tenant_id / \"settings.json\"\n",
        "        path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        with open(path, \"w\") as f:\n",
        "            json.dump(settings.dict(), f, indent=2)\n",
        "\n",
        "# Streamlit UI\n",
        "def admin_console_ui():\n",
        "    \"\"\"Streamlit UI for admin console.\"\"\"\n",
        "    st.set_page_config(page_title=\"LedgerOne Admin\", layout=\"wide\")\n",
        "    if \"token\" not in st.session_state:\n",
        "        st.error(\"Please log in\")\n",
        "        return\n",
        "    current_user = get_current_user(st.session_state[\"token\"])\n",
        "    console = AdminConsole(tenant_id=current_user[\"tenant_id\"])\n",
        "    st.title(f\"Admin Console - {current_user['tenant_id']}\")\n",
        "\n",
        "    if current_user[\"role\"] == RoleEnum.SUPER_ADMIN:\n",
        "        st.header(\"Super Admin Controls\")\n",
        "        user_id = st.text_input(\"Impersonate User ID\")\n",
        "        if st.button(\"Impersonate\"):\n",
        "            try:\n",
        "                user_data = console.impersonate_user(user_id, current_user)\n",
        "                st.success(f\"Impersonated {user_id}\")\n",
        "                st.json(user_data)\n",
        "            except LedgerOneError as e:\n",
        "                st.error(str(e))\n",
        "        if st.button(\"Trigger Global Retrain\"):\n",
        "            try:\n",
        "                console.trigger_retrain(current_user)\n",
        "                st.success(\"Global retrain triggered\")\n",
        "            except LedgerOneError as e:\n",
        "                st.error(str(e))\n",
        "\n",
        "    if console._check_admin_privileges(current_user[\"role\"]):\n",
        "        st.header(\"Tenant Admin Controls\")\n",
        "        user_id = st.text_input(\"User ID for Role Assignment\")\n",
        "        role = st.selectbox(\"Role\", [r.value for r in RoleEnum])\n",
        "        if st.button(\"Assign Role\"):\n",
        "            try:\n",
        "                console.assign_role(user_id, RoleEnum(role), current_user)\n",
        "                st.success(f\"Assigned {role} to {user_id}\")\n",
        "            except LedgerOneError as e:\n",
        "                st.error(str(e))\n",
        "\n",
        "        threshold = st.number_input(\"Approval Threshold (KES)\", min_value=0.0, value=10000.0)\n",
        "        if st.button(\"Set Threshold\"):\n",
        "            try:\n",
        "                console.set_approval_threshold(threshold, current_user)\n",
        "                st.success(f\"Set threshold to {threshold} KES\")\n",
        "            except LedgerOneError as e:\n",
        "                st.error(str(e))\n",
        "\n",
        "        connector = st.selectbox(\"Connector\", [\"mpesa\", \"bank_csv\", \"tesseract\"])\n",
        "        enable = st.checkbox(\"Enable Connector\", value=True)\n",
        "        if st.button(\"Toggle Connector\"):\n",
        "            try:\n",
        "                console.toggle_connector(connector, enable, current_user)\n",
        "                st.success(f\"Connector {connector} {'enabled' if enable else 'disabled'}\")\n",
        "            except LedgerOneError as e:\n",
        "                st.error(str(e))\n",
        "\n",
        "# FastAPI endpoints\n",
        "from fastapi import HTTPException\n",
        "@app.post(\"/admin/assign_role\")\n",
        "async def assign_role_endpoint(user_id: str, role: RoleEnum, current_user: dict = Depends(get_current_user)):\n",
        "    \"\"\"Assign role to user.\"\"\"\n",
        "    try:\n",
        "        console = AdminConsole(tenant_id=current_user[\"tenant_id\"])\n",
        "        console.assign_role(user_id, role, current_user)\n",
        "        return {\"status\": \"role assigned\"}\n",
        "    except LedgerOneError as e:\n",
        "        raise HTTPException(status_code=403, detail=str(e))\n",
        "\n",
        "@app.post(\"/admin/set_threshold\")\n",
        "async def set_threshold_endpoint(threshold: float, current_user: dict = Depends(get_current_user)):\n",
        "    \"\"\"Set approval threshold.\"\"\"\n",
        "    try:\n",
        "        console = AdminConsole(tenant_id=current_user[\"tenant_id\"])\n",
        "        console.set_approval_threshold(threshold, current_user)\n",
        "        return {\"status\": \"threshold set\"}\n",
        "    except LedgerOneError as e:\n",
        "        raise HTTPException(status_code=403, detail=str(e))\n",
        "\n",
        "@app.post(\"/admin/toggle_connector\")\n",
        "async def toggle_connector_endpoint(connector: str, enable: bool, current_user: dict = Depends(get_current_user)):\n",
        "    \"\"\"Toggle connector.\"\"\"\n",
        "    try:\n",
        "        console = AdminConsole(tenant_id=current_user[\"tenant_id\"])\n",
        "        console.toggle_connector(connector, enable, current_user)\n",
        "        return {\"status\": f\"connector {connector} {'enabled' if enable else 'disabled'}\"}\n",
        "    except LedgerOneError as e:\n",
        "        raise HTTPException(status_code=403, detail=str(e))\n",
        "\n",
        "@app.post(\"/admin/retrain\")\n",
        "async def retrain_endpoint(current_user: dict = Depends(get_current_user)):\n",
        "    \"\"\"Trigger model retrain.\"\"\"\n",
        "    try:\n",
        "        console = AdminConsole(tenant_id=current_user[\"tenant_id\"])\n",
        "        console.trigger_retrain(current_user)\n",
        "        return {\"status\": \"retrain triggered\"}\n",
        "    except LedgerOneError as e:\n",
        "        raise HTTPException(status_code=403, detail=str(e))\n",
        "\n",
        "# Unit tests\n",
        "def test_role_assignment(tmp_path):\n",
        "    \"\"\"Test role assignment.\"\"\"\n",
        "    SETTINGS_DIR = tmp_path / \"settings\"\n",
        "    SETTINGS_DIR.mkdir(parents=True)\n",
        "    user = User(user_id=\"test_001\", username=\"testuser\", password_hash=\"hash\", role=RoleEnum.AP_CLERK, tenant_id=\"test_tenant\")\n",
        "    with open(SETTINGS_DIR / \"test_tenant\" / \"users.json\", \"w\") as f:\n",
        "        json.dump([user.dict()], f)\n",
        "    console = AdminConsole(tenant_id=\"test_tenant\")\n",
        "    console.assign_role(\"test_001\", RoleEnum.FINANCE_MGR, {\"role\": RoleEnum.COMPANY_ADMIN, \"tenant_id\": \"test_tenant\"})\n",
        "    updated_user = console._load_user(\"test_001\")\n",
        "    assert updated_user.role == RoleEnum.FINANCE_MGR\n",
        "\n",
        "def test_threshold_setting(tmp_path):\n",
        "    \"\"\"Test approval threshold setting.\"\"\"\n",
        "    SETTINGS_DIR = tmp_path / \"settings\"\n",
        "    SETTINGS_DIR.mkdir(parents=True)\n",
        "    console = AdminConsole(tenant_id=\"test_tenant\")\n",
        "    console.set_approval_threshold(20000.0, {\"role\": RoleEnum.COMPANY_ADMIN, \"tenant_id\": \"test_tenant\"})\n",
        "    settings = console._load_settings()\n",
        "    assert settings.approval_threshold == 20000.0\n",
        "\n",
        "# Smoke test\n",
        "def run_smoke_test():\n",
        "    \"\"\"Run admin console smoke tests.\"\"\"\n",
        "    import tempfile\n",
        "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
        "        global SETTINGS_DIR\n",
        "        SETTINGS_DIR = Path(tmpdirname) / \"settings\"\n",
        "        test_role_assignment(Path(tmpdirname))\n",
        "        test_threshold_setting(Path(tmpdirname))\n",
        "    print(\"Admin console smoke tests passed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_smoke_test()"
      ],
      "metadata": {
        "id": "iwwgTgmNVvqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cell_09_connectors_fallbacks.py\n",
        "\"\"\"\n",
        "LedgerOne Connectors and Fallbacks\n",
        "=================================\n",
        "Implements connectors for external services (M-Pesa, bank APIs, OCR) with graceful fallbacks.\n",
        "Ensures Kenyan-specific integrations (M-Pesa Daraja, bank CSV formats). Logs provider usage.\n",
        "Key features:\n",
        "- Connectors: M-Pesa (Daraja API), bank CSV, cloud OCR (Tesseract fallback).\n",
        "- Facade pattern: Tries preferred provider, falls back to local alternative.\n",
        "- Metadata logging to data/audit/{tenant}/.\n",
        "- Kenyan-specific: M-Pesa transaction codes, bank CSV formats (KCB, Equity).\n",
        "- Unit tests for connectors and fallbacks.\n",
        "\n",
        "Configuration:\n",
        "- Set MPESA_KEY, OCR_API_KEY in .env (falls back if unset).\n",
        "- Connector status in data/settings/{tenant}/.\n",
        "\n",
        "Extension points:\n",
        "- Add new connectors in ConnectorFacade.\n",
        "- Extend fallback logic in Connector classes.\n",
        "\"\"\"\n",
        "import json\n",
        "import csv\n",
        "from pathlib import Path\n",
        "from typing import Dict, Optional, List\n",
        "from datetime import datetime\n",
        "import logging\n",
        "import requests\n",
        "import pytesseract\n",
        "from pdf2image import convert_from_path\n",
        "from ledgerone_prototype.cell_01_core_bootstrap import setup_logging, LedgerOneError, config\n",
        "from ledgerone_prototype.cell_03_canonical_schema_featurestore import CanonicalDocument\n",
        "\n",
        "# File paths\n",
        "CONNECTOR_LOGS_DIR = Path(config.DATA_DIR) / \"connector_logs\"\n",
        "CONNECTOR_LOGS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Connector models\n",
        "class ConnectorResponse(BaseModel):\n",
        "    data: Dict[str, Any]\n",
        "    provider_used: str\n",
        "    status: str\n",
        "    errors: List[str] = []\n",
        "\n",
        "class ConnectorFacade:\n",
        "    \"\"\"Manages connectors with fallbacks.\"\"\"\n",
        "    def __init__(self, tenant_id: str):\n",
        "        self.tenant_id = tenant_id\n",
        "        self.logger = setup_logging(tenant_id)\n",
        "        self.connectors = {\n",
        "            \"mpesa\": MpesaConnector(tenant_id),\n",
        "            \"bank_csv\": BankCsvConnector(tenant_id),\n",
        "            \"ocr\": OcrConnector(tenant_id),\n",
        "        }\n",
        "\n",
        "    def process(self, connector_type: str, input_data: Any) -> ConnectorResponse:\n",
        "        \"\"\"Process input using specified connector with fallback.\"\"\"\n",
        "        try:\n",
        "            connector = self.connectors.get(connector_type)\n",
        "            if not connector:\n",
        "                raise LedgerOneError(f\"Unknown connector: {connector_type}\")\n",
        "            response = connector.process(input_data)\n",
        "            self.logger.info(f\"Connector {connector_type} processed\", extra={\"action\": \"process_connector\", \"provider\": response.provider_used})\n",
        "            return response\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Connector {connector_type} failed: {str(e)}\", extra={\"action\": \"process_connector\"})\n",
        "            raise LedgerOneError(f\"Connector {connector_type} failed: {str(e)}\")\n",
        "\n",
        "class MpesaConnector:\n",
        "    \"\"\"M-Pesa connector with CSV fallback.\"\"\"\n",
        "    def __init__(self, tenant_id: str):\n",
        "        self.tenant_id = tenant_id\n",
        "        self.logger = setup_logging(tenant_id)\n",
        "\n",
        "    def process(self, file_path: str) -> ConnectorResponse:\n",
        "        \"\"\"Process M-Pesa transactions.\"\"\"\n",
        "        try:\n",
        "            if config.MPESA_KEY:\n",
        "                return self._process_api(file_path)\n",
        "            return self._process_csv(file_path)\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"M-Pesa processing failed: {str(e)}\", extra={\"action\": \"process_mpesa\"})\n",
        "            return ConnectorResponse(data={}, provider_used=\"none\", status=\"failed\", errors=[str(e)])\n",
        "\n",
        "    def _process_api(self, file_path: str) -> ConnectorResponse:\n",
        "        \"\"\"Process M-Pesa via Daraja API.\"\"\"\n",
        "        # Source: https://developer.safaricom.co.ke/ (2025 Daraja API)\n",
        "        try:\n",
        "            response = requests.get(\n",
        "                \"https://api.safaricom.co.ke/mpesa/transaction\",\n",
        "                headers={\"Authorization\": f\"Bearer {config.MPESA_KEY}\"},\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "            self.logger.info(\"M-Pesa API success\", extra={\"action\": \"process_mpesa_api\"})\n",
        "            return ConnectorResponse(data=data, provider_used=\"mpesa_api\", status=\"success\")\n",
        "        except Exception as e:\n",
        "            self.logger.warning(f\"M-Pesa API failed, falling back: {str(e)}\", extra={\"action\": \"process_mpesa_api\"})\n",
        "            return self._process_csv(file_path)\n",
        "\n",
        "    def _process_csv(self, file_path: str) -> ConnectorResponse:\n",
        "        \"\"\"Fallback: Parse M-Pesa CSV.\"\"\"\n",
        "        try:\n",
        "            with open(file_path) as f:\n",
        "                reader = csv.DictReader(f)\n",
        "                transactions = [\n",
        "                    {\n",
        "                        \"doc_id\": f\"mpesa_{hashlib.md5(str(row).encode()).hexdigest()}\",\n",
        "                        \"tenant_id\": self.tenant_id,\n",
        "                        \"source_type\": \"bank_statement\",\n",
        "                        \"description\": row[\"Details\"],\n",
        "                        \"amount\": float(row[\"Amount\"]),\n",
        "                        \"date\": row[\"Date\"],\n",
        "                        \"mpesa_transaction_id\": row[\"Receipt No\"],\n",
        "                    } for row in reader\n",
        "                ]\n",
        "            self.logger.info(\"M-Pesa CSV parsed\", extra={\"action\": \"process_mpesa_csv\"})\n",
        "            return ConnectorResponse(data={\"records\": transactions}, provider_used=\"mpesa_csv\", status=\"success\")\n",
        "        except Exception as e:\n",
        "            raise LedgerOneError(f\"M-Pesa CSV parse failed: {str(e)}\")\n",
        "\n",
        "class BankCsvConnector:\n",
        "    \"\"\"Bank CSV connector.\"\"\"\n",
        "    def __init__(self, tenant_id: str):\n",
        "        self.tenant_id = tenant_id\n",
        "        self.logger = setup_logging(tenant_id)\n",
        "\n",
        "    def process(self, file_path: str) -> ConnectorResponse:\n",
        "        \"\"\"Parse Kenyan bank CSV (e.g., KCB, Equity).\"\"\"\n",
        "        # Source: https://www.kcbgroup.com/ (2025 CSV formats)\n",
        "        try:\n",
        "            with open(file_path) as f:\n",
        "                reader = csv.DictReader(f)\n",
        "                transactions = [\n",
        "                    {\n",
        "                        \"doc_id\": f\"bank_{hashlib.md5(str(row).encode()).hexdigest()}\",\n",
        "                        \"tenant_id\": self.tenant_id,\n",
        "                        \"source_type\": \"bank_statement\",\n",
        "                        \"description\": row[\"Description\"],\n",
        "                        \"amount\": float(row[\"Credit\"] or 0) - float(row[\"Debit\"] or 0),\n",
        "                        \"date\": row[\"Date\"],\n",
        "                    } for row in reader\n",
        "                ]\n",
        "            self.logger.info(\"Bank CSV parsed\", extra={\"action\": \"process_bank_csv\"})\n",
        "            return ConnectorResponse(data={\"records\": transactions}, provider_used=\"bank_csv\", status=\"success\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Bank CSV parse failed: {str(e)}\", extra={\"action\": \"process_bank_csv\"})\n",
        "            return ConnectorResponse(data={}, provider_used=\"bank_csv\", status=\"failed\", errors=[str(e)])\n",
        "\n",
        "class OcrConnector:\n",
        "    \"\"\"OCR connector with Tesseract fallback.\"\"\"\n",
        "    def __init__(self, tenant_id: str):\n",
        "        self.tenant_id = tenant_id\n",
        "        self.logger = setup_logging(tenant_id)\n",
        "\n",
        "    def process(self, file_path: str) -> ConnectorResponse:\n",
        "        \"\"\"Process PDF/image via OCR.\"\"\"\n",
        "        try:\n",
        "            if config.OCR_API_KEY:\n",
        "                return self._process_cloud_ocr(file_path)\n",
        "            return self._process_tesseract(file_path)\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"OCR processing failed: {str(e)}\", extra={\"action\": \"process_ocr\"})\n",
        "            return ConnectorResponse(data={}, provider_used=\"none\", status=\"failed\", errors=[str(e)])\n",
        "\n",
        "    def _process_cloud_ocr(self, file_path: str) -> ConnectorResponse:\n",
        "        \"\"\"Process via cloud OCR API (stub).\"\"\"\n",
        "        try:\n",
        "            # Placeholder for cloud OCR (e.g., Google Vision)\n",
        "            self.logger.warning(\"Cloud OCR not implemented, falling back to Tesseract\", extra={\"action\": \"process_cloud_ocr\"})\n",
        "            return self._process_tesseract(file_path)\n",
        "        except Exception as e:\n",
        "            self.logger.warning(f\"Cloud OCR failed, falling back: {str(e)}\", extra={\"action\": \"process_cloud_ocr\"})\n",
        "            return self._process_tesseract(file_path)\n",
        "\n",
        "    def _process_tesseract(self, file_path: str) -> ConnectorResponse:\n",
        "        \"\"\"Process via Tesseract OCR.\"\"\"\n",
        "        try:\n",
        "            images = convert_from_path(file_path) if file_path.endswith(\".pdf\") else [Image.open(file_path)]\n",
        "            text = \"\"\n",
        "            for img in images:\n",
        "                text += pytesseract.image_to_string(img)\n",
        "            data = {\n",
        "                \"doc_id\": f\"ocr_{hashlib.md5(text.encode()).hexdigest()}\",\n",
        "                \"tenant_id\": self.tenant_id,\n",
        "                \"source_type\": \"invoice\",\n",
        "                \"text\": text,\n",
        "            }\n",
        "            self.logger.info(\"Tesseract OCR parsed\", extra={\"action\": \"process_tesseract\"})\n",
        "            return ConnectorResponse(data=data, provider_used=\"tesseract\", status=\"success\")\n",
        "        except Exception as e:\n",
        "            raise LedgerOneError(f\"Tesseract OCR failed: {str(e)}\")\n",
        "\n",
        "# FastAPI endpoints\n",
        "from fastapi import HTTPException, UploadFile, File\n",
        "@app.post(\"/connectors/process/{connector_type}\")\n",
        "async def process_connector_endpoint(connector_type: str, file: UploadFile = File(...), tenant_id: str = Depends(get_current_user)):\n",
        "    \"\"\"Process file via connector.\"\"\"\n",
        "    try:\n",
        "        facade = ConnectorFacade(tenant_id=tenant_id[\"tenant_id\"])\n",
        "        file_path = CONNECTOR_LOGS_DIR / tenant_id[\"tenant_id\"] / file.filename\n",
        "        file_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        with open(file_path, \"wb\") as f:\n",
        "            f.write(await file.read())\n",
        "        response = facade.process(connector_type, str(file_path))\n",
        "        return response.dict()\n",
        "    except LedgerOneError as e:\n",
        "        raise HTTPException(status_code=400, detail=str(e))\n",
        "\n",
        "# Unit tests\n",
        "def test_mpesa_connector(tmp_path):\n",
        "    \"\"\"Test M-Pesa connector with CSV fallback.\"\"\"\n",
        "    csv_content = \"Date,Details,Amount,Receipt No\\n2025-09-01,Payment,1000,MP123456\"\n",
        "    file_path = tmp_path / \"mpesa.csv\"\n",
        "    file_path.write_text(csv_content)\n",
        "    connector = MpesaConnector(tenant_id=\"test_tenant\")\n",
        "    response = connector.process(str(file_path))\n",
        "    assert response.status == \"success\"\n",
        "    assert response.provider_used == \"mpesa_csv\"\n",
        "    assert len(response.data[\"records\"]) == 1\n",
        "\n",
        "def test_bank_csv_connector(tmp_path):\n",
        "    \"\"\"Test bank CSV connector.\"\"\"\n",
        "    csv_content = \"Date,Description,Debit,Credit\\n2025-09-01,Payment,1000,\"\n",
        "    file_path = tmp_path / \"bank.csv\"\n",
        "    file_path.write_text(csv_content)\n",
        "    connector = BankCsvConnector(tenant_id=\"test_tenant\")\n",
        "    response = connector.process(str(file_path))\n",
        "    assert response.status == \"success\"\n",
        "    assert response.provider_used == \"bank_csv\"\n",
        "    assert len(response.data[\"records\"]) == 1\n",
        "\n",
        "# Smoke test\n",
        "def run_smoke_test():\n",
        "    \"\"\"Run connectors smoke tests.\"\"\"\n",
        "    import tempfile\n",
        "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
        "        global CONNECTOR_LOGS_DIR\n",
        "        CONNECTOR_LOGS_DIR = Path(tmpdirname) / \"connector_logs\"\n",
        "        test_mpesa_connector(Path(tmpdirname))\n",
        "        test_bank_csv_connector(Path(tmpdirname))\n",
        "    print(\"Connectors smoke tests passed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_smoke_test()"
      ],
      "metadata": {
        "id": "FZt3G4ulV8Br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cell_10_faker_tests_deploy.py\n",
        "\"\"\"\n",
        "LedgerOne Faker, Tests, and Deployment\n",
        "=====================================\n",
        "Generates demo data for three Kenyan-specific tenants, runs tests with auto-fix, and provides\n",
        "deployment scripts. Seeds demo users and realistic datasets (invoices, payroll).\n",
        "Key features:\n",
        "- Faker: 50 invoices, 1 payroll run per tenant (manufacturing, service, retail).\n",
        "- Demo users: 3 tenants with roles, 1 super admin, credentials in data/DEMO_ACCOUNTS.md.\n",
        "- Test runner: Runs unit tests, auto-fixes minor errors (linting, imports).\n",
        "- Deployment: run_app.py, Dockerfile, requirements.txt, run_tests_and_fix.sh.\n",
        "- Kenyan-specific: Invoice/payroll data from KNBS distributions.\n",
        "- Unit tests for faker and deployment.\n",
        "\n",
        "Configuration:\n",
        "- Uses constants from cell_01_core_bootstrap.py.\n",
        "- Demo data in data/demo/{tenant}/.\n",
        "\n",
        "Extension points:\n",
        "- Add new tenant types in seed_tenants().\n",
        "- Extend faker distributions in generate_invoices/payroll().\n",
        "\"\"\"\n",
        "import json\n",
        "import csv\n",
        "from pathlib import Path\n",
        "from typing import List, Dict\n",
        "from datetime import datetime, timedelta\n",
        "import logging\n",
        "import subprocess\n",
        "from faker import Faker\n",
        "from ledgerone_prototype.cell_01_core_bootstrap import setup_logging, LedgerOneError, config, RoleEnum, User, hash_password\n",
        "from ledgerone_prototype.cell_03_canonical_schema_featurestore import CanonicalDocument, Vendor, LineItem, VatLine\n",
        "import pandas as pd\n",
        "import pytest\n",
        "\n",
        "# File paths\n",
        "DEMO_DIR = Path(config.DATA_DIR) / \"demo\"\n",
        "DEMO_DIR.mkdir(parents=True, exist_ok=True)\n",
        "DEMO_ACCOUNTS_MD = DEMO_DIR / \"DEMO_ACCOUNTS.md\"\n",
        "\n",
        "# Faker setup\n",
        "fake = Faker(\"en_US\")\n",
        "Faker.seed(12345)\n",
        "\n",
        "class DemoSeeder:\n",
        "    \"\"\"Generates demo data for tenants.\"\"\"\n",
        "    def __init__(self, tenant_id: str):\n",
        "        self.tenant_id = tenant_id\n",
        "        self.logger = setup_logging(tenant_id)\n",
        "        self.vendors = [\"Safaricom\", \"KPLC\", \"Nairobi Water\", \"Jumia Kenya\", \"Carrefour\"]  # KNBS-derived\n",
        "\n",
        "    def generate_invoices(self, count: int = 50) -> List[CanonicalDocument]:\n",
        "        \"\"\"Generate realistic invoices (KES 50k–500k, KNBS data).\"\"\"\n",
        "        invoices = []\n",
        "        for _ in range(count):\n",
        "            total = fake.random_int(min=50000, max=500000)\n",
        "            doc = CanonicalDocument(\n",
        "                doc_id=f\"inv_{fake.uuid4()}\",\n",
        "                tenant_id=self.tenant_id,\n",
        "                source_type=\"invoice\",\n",
        "                vendor=Vendor(raw=fake.random_element(self.vendors)),\n",
        "                lines=[LineItem(description=fake.text(max_nb_chars=50), quantity=1, unit_price=total, total=total)],\n",
        "                total=total,\n",
        "                vat_lines=[VatLine(amount=total * config.KENYA_CONSTANTS[\"VAT_RATE\"])],\n",
        "                currency=\"KES\",\n",
        "                dates={\"issue\": fake.date_this_year().isoformat(), \"due\": (fake.date_this_year() + timedelta(days=30)).isoformat()},\n",
        "                parsed_by=\"faker\",\n",
        "                confidence=1.0,\n",
        "            )\n",
        "            invoices.append(doc)\n",
        "            self._save_document(doc)\n",
        "        self.logger.info(f\"Generated {count} invoices\", extra={\"action\": \"generate_invoices\"})\n",
        "        return invoices\n",
        "\n",
        "    def generate_payroll(self) -> List[CanonicalDocument]:\n",
        "        \"\"\"Generate payroll run (KES 30k–100k, KNBS data).\"\"\"\n",
        "        payroll = []\n",
        "        for _ in range(10):  # 10 employees\n",
        "            basic_pay = fake.random_int(min=30000, max=100000)\n",
        "            doc = CanonicalDocument(\n",
        "                doc_id=f\"pay_{fake.uuid4()}\",\n",
        "                tenant_id=self.tenant_id,\n",
        "                source_type=\"payroll\",\n",
        "                vendor=Vendor(raw=fake.name()),\n",
        "                lines=[LineItem(description=\"Salary\", quantity=1, unit_price=basic_pay, total=basic_pay)],\n",
        "                total=basic_pay,\n",
        "                currency=\"KES\",\n",
        "                dates={\"payroll_date\": fake.date_this_month().isoformat()},\n",
        "                parsed_by=\"faker\",\n",
        "                confidence=1.0,\n",
        "                employee_id=f\"E{fake.random_int(min=100, max=999)}\",\n",
        "            )\n",
        "            payroll.append(doc)\n",
        "            self._save_document(doc)\n",
        "        self.logger.info(\"Generated payroll run\", extra={\"action\": \"generate_payroll\"})\n",
        "        return payroll\n",
        "\n",
        "    def _save_document(self, doc: CanonicalDocument):\n",
        "        \"\"\"Save document to JSON.\"\"\"\n",
        "        path = DEMO_DIR / self.tenant_id / f\"{doc.doc_id}.json\"\n",
        "        path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        with open(path, \"w\") as f:\n",
        "            json.dump(doc.dict(), f, indent=2)\n",
        "\n",
        "def seed_tenants():\n",
        "    \"\"\"Seed three tenants with users and data.\"\"\"\n",
        "    tenants = [\n",
        "        {\"id\": \"tenant_demo_1\", \"type\": \"manufacturing\", \"roles\": [RoleEnum.CEO, RoleEnum.CFO, RoleEnum.FINANCE_MGR, RoleEnum.AP_CLERK, RoleEnum.HR_MGR, RoleEnum.BRANCH_MGR]},\n",
        "        {\"id\": \"tenant_demo_2\", \"type\": \"service\", \"roles\": [RoleEnum.CEO, RoleEnum.FINANCE_MGR, RoleEnum.AP_CLERK]},\n",
        "        {\"id\": \"tenant_demo_3\", \"type\": \"retail\", \"roles\": [RoleEnum.CEO, RoleEnum.FINANCE_MGR, RoleEnum.BRANCH_MGR]},\n",
        "    ]\n",
        "    users = [\n",
        "        User(user_id=\"super_001\", username=\"superadmin\", password_hash=hash_password(\"Super123!\"), role=RoleEnum.SUPER_ADMIN, tenant_id=\"global\"),\n",
        "    ]\n",
        "    credentials_md = \"# Demo Accounts\\n\\n| Tenant | Username | Password | Role |\\n|--------|----------|----------|------|\\n\"\n",
        "    credentials_md += f\"| global | superadmin | Super123! | {RoleEnum.SUPER_ADMIN} |\\n\"\n",
        "\n",
        "    for tenant in tenants:\n",
        "        seeder = DemoSeeder(tenant[\"id\"])\n",
        "        seeder.generate_invoices()\n",
        "        seeder.generate_payroll()\n",
        "        tenant_users = [\n",
        "            User(user_id=f\"{tenant['id']}_admin\", username=f\"{tenant['id']}_admin\", password_hash=hash_password(\"Admin123!\"), role=RoleEnum.COMPANY_ADMIN, tenant_id=tenant[\"id\"]),\n",
        "        ]\n",
        "        for role in tenant[\"roles\"]:\n",
        "            user_id = f\"{tenant['id']}_{role}_{fake.uuid4()[:8]}\"\n",
        "            username = f\"{tenant['id']}_{role}\"\n",
        "            tenant_users.append(User(user_id=user_id, username=username, password_hash=hash_password(\"User123!\"), role=role, tenant_id=tenant[\"id\"]))\n",
        "            credentials_md += f\"| {tenant['id']} | {username} | User123! | {role} |\\n\"\n",
        "        users.extend(tenant_users)\n",
        "        path = DEMO_DIR / tenant[\"id\"] / \"users.json\"\n",
        "        path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        with open(path, \"w\") as f:\n",
        "            json.dump([u.dict() for u in tenant_users], f, indent=2)\n",
        "\n",
        "    with open(DEMO_DIR / \"global\" / \"users.json\", \"w\") as f:\n",
        "        json.dump([u.dict() for u in users if u.tenant_id == \"global\"], f, indent=2)\n",
        "    with open(DEMO_ACCOUNTS_MD, \"w\") as f:\n",
        "        f.write(credentials_md)\n",
        "\n",
        "# Test runner\n",
        "def run_tests_and_fix():\n",
        "    \"\"\"Run tests and attempt auto-fixes.\"\"\"\n",
        "    logger = setup_logging(\"test\")\n",
        "    try:\n",
        "        result = subprocess.run([\"pytest\", \"-v\"], capture_output=True, text=True)\n",
        "        if result.returncode != 0:\n",
        "            logger.warning(f\"Tests failed: {result.stderr}\", extra={\"action\": \"run_tests\"})\n",
        "            # Attempt auto-fix (e.g., linting)\n",
        "            try:\n",
        "                subprocess.run([\"black\", \".\"], check=True)\n",
        "                subprocess.run([\"isort\", \".\"], check=True)\n",
        "                logger.info(\"Applied auto-fixes\", extra={\"action\": \"auto_fix\"})\n",
        "                result = subprocess.run([\"pytest\", \"-v\"], capture_output=True, text=True)\n",
        "                if result.returncode == 0:\n",
        "                    logger.info(\"Tests passed after auto-fix\", extra={\"action\": \"run_tests\"})\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Auto-fix failed: {str(e)}\", extra={\"action\": \"auto_fix\"})\n",
        "        return result.stdout\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Test runner failed: {str(e)}\", extra={\"action\": \"run_tests\"})\n",
        "        raise LedgerOneError(f\"Test runner failed: {str(e)}\")\n",
        "\n",
        "# Deployment scripts\n",
        "def generate_run_app():\n",
        "    \"\"\"Generate run_app.py.\"\"\"\n",
        "    run_app_content = \"\"\"\n",
        "import streamlit as st\n",
        "from ledgerone_prototype.cell_01_core_bootstrap import run_streamlit\n",
        "from ledgerone_prototype.cell_08_admin_console import admin_console_ui\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_streamlit()\n",
        "    admin_console_ui()\n",
        "\"\"\"\n",
        "    with open(BASE_DIR / \"run_app.py\", \"w\") as f:\n",
        "        f.write(run_app_content)\n",
        "\n",
        "def generate_requirements():\n",
        "    \"\"\"Generate requirements.txt.\"\"\"\n",
        "    requirements = \"\"\"\n",
        "fastapi==0.115.0\n",
        "streamlit==1.38.0\n",
        "pydantic==2.9.2\n",
        "pandas==2.2.3\n",
        "sqlalchemy==2.0.35\n",
        "pytesseract==0.3.13\n",
        "pdf2image==1.17.0\n",
        "sentence-transformers==3.1.1\n",
        "scikit-learn==1.5.2\n",
        "lightgbm==4.5.0\n",
        "pytest==8.3.3\n",
        "black==24.8.0\n",
        "isort==5.13.2\n",
        "bcrypt==4.2.0\n",
        "pyjwt==2.9.0\n",
        "requests==2.32.3\n",
        "\"\"\"\n",
        "    with open(BASE_DIR / \"requirements.txt\", \"w\") as f:\n",
        "        f.write(requirements)\n",
        "\n",
        "def generate_dockerfile():\n",
        "    \"\"\"Generate Dockerfile.\"\"\"\n",
        "    dockerfile = \"\"\"\n",
        "FROM python:3.11-slim\n",
        "WORKDIR /app\n",
        "COPY requirements.txt .\n",
        "RUN pip install -r requirements.txt\n",
        "COPY . .\n",
        "CMD [\"streamlit\", \"run\", \"run_app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\"]\n",
        "\"\"\"\n",
        "    with open(BASE_DIR / \"Dockerfile\", \"w\") as f:\n",
        "        f.write(dockerfile)\n",
        "\n",
        "def generate_test_script():\n",
        "    \"\"\"Generate run_tests_and_fix.sh.\"\"\"\n",
        "    test_script = \"\"\"\n",
        "#!/bin/bash\n",
        "pytest -v\n",
        "if [ $? -ne 0 ]; then\n",
        "    echo \"Tests failed, attempting auto-fix...\"\n",
        "    black .\n",
        "    isort .\n",
        "    pytest -v\n",
        "fi\n",
        "\"\"\"\n",
        "    with open(BASE_DIR / \"run_tests_and_fix.sh\", \"w\") as f:\n",
        "        f.write(test_script)\n",
        "    subprocess.run([\"chmod\", \"+x\", str(BASE_DIR / \"run_tests_and_fix.sh\")])\n",
        "\n",
        "# Unit tests\n",
        "def test_faker_invoices(tmp_path):\n",
        "    \"\"\"Test invoice generation.\"\"\"\n",
        "    DEMO_DIR = tmp_path / \"demo\"\n",
        "    DEMO_DIR.mkdir(parents=True)\n",
        "    seeder = DemoSeeder(tenant_id=\"test_tenant\")\n",
        "    invoices = seeder.generate_invoices(5)\n",
        "    assert len(invoices) == 5\n",
        "    assert all(doc.source_type == \"invoice\" for doc in invoices)\n",
        "    assert (DEMO_DIR / \"test_tenant\").exists()\n",
        "\n",
        "def test_seed_tenants(tmp_path):\n",
        "    \"\"\"Test tenant seeding.\"\"\"\n",
        "    DEMO_DIR = tmp_path / \"demo\"\n",
        "    DEMO_ACCOUNTS_MD = DEMO_DIR / \"DEMO_ACCOUNTS.md\"\n",
        "    seed_tenants()\n",
        "    assert DEMO_ACCOUNTS_MD.exists()\n",
        "    assert (DEMO_DIR / \"tenant_demo_1\" / \"users.json\").exists()\n",
        "\n",
        "# Smoke test\n",
        "def run_smoke_test():\n",
        "    \"\"\"Run faker and deployment smoke tests.\"\"\"\n",
        "    import tempfile\n",
        "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
        "        global DEMO_DIR, DEMO_ACCOUNTS_MD\n",
        "        DEMO_DIR = Path(tmpdirname) / \"demo\"\n",
        "        DEMO_ACCOUNTS_MD = DEMO_DIR / \"DEMO_ACCOUNTS.md\"\n",
        "        test_faker_invoices(Path(tmpdirname))\n",
        "        test_seed_tenants(Path(tmpdirname))\n",
        "    print(\"Faker and deployment smoke tests passed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    seed_tenants()\n",
        "    generate_run_app()\n",
        "    generate_requirements()\n",
        "    generate_dockerfile()\n",
        "    generate_test_script()\n",
        "    run_smoke_test()"
      ],
      "metadata": {
        "id": "kaJLtJhWWJmL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}